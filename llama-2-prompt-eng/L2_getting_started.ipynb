{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac959c4b-d7a9-4949-9c39-2ab46fbf82cd",
   "metadata": {},
   "source": [
    "# Lesson 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6df1e2b-2079-4dee-84e9-77e13dd31e05",
   "metadata": {},
   "source": [
    "### Getting started with Llama 2\n",
    "\n",
    "The code to call the Llama 2 models through the Together.ai hosted API service has been wrapped into a helper function called `llama`. You can take a look at this code if you like by opening the utils.py file using the File -> Open menu item above this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10075542-6019-40b6-b3c1-532383e4a6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import llama helper function\n",
    "from utils import llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a549fd-dd50-4ae1-a5d8-ce00fdc707a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the prompt\n",
    "prompt = \"Help me write a birthday card for my dear friend Andrew.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d46c4b-2926-44a9-9e25-befb6bd6648c",
   "metadata": {},
   "source": [
    "**Note:** LLMs can have different responses for the same prompt, which is why throughout the course, the responses you get might be slightly different than the ones in the lecture videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89f4c48-363d-4ddb-8f20-215bbd47004a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass prompt to the llama function, store output as 'response' then print\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a573ca3f-9ffc-4f08-91eb-7e88d99d2f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set verbose to True to see the full prompt that is passed to the model.\n",
    "prompt = \"Help me write a birthday card for my dear friend Andrew.\"\n",
    "response = llama(prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0c9bc9-9777-45bb-9e52-1cd99a42e716",
   "metadata": {},
   "source": [
    "### Chat vs. base models\n",
    "\n",
    "Ask model a simple question to demonstrate the different behavior of chat vs. base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37cc261-a9d5-486c-a0f1-7f0ce93f403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### chat model\n",
    "prompt = \"What is the capital of France?\"\n",
    "response = llama(prompt, \n",
    "                 verbose=True,\n",
    "                 model=\"togethercomputer/llama-2-7b-chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d596d0-9469-4220-a03a-b50b0226776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bef6df-ce83-4be7-98a7-cd0c353c4188",
   "metadata": {},
   "outputs": [],
   "source": [
    "### base model\n",
    "prompt = \"What is the capital of France?\"\n",
    "response = llama(prompt, \n",
    "                 verbose=True,\n",
    "                 add_inst=False,\n",
    "                 model=\"togethercomputer/llama-2-7b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bf01a1-2c3e-4073-a4b7-546f5de0e5a1",
   "metadata": {},
   "source": [
    "Note how the prompt **does not** include the `[INST]` and `[/INST]` tags as `add_inst` was set to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1941be3a-de66-4d08-808e-f93a0393f864",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69ac25a-95cd-43da-91e4-2d3c885af811",
   "metadata": {},
   "source": [
    "### Changing the temperature setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c11476-c128-4370-9405-e2899c0284ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Help me write a birthday card for my dear friend Andrew.\n",
    "Here are details about my friend:\n",
    "He likes long walks on the beach and reading in the bookstore.\n",
    "His hobbies include reading research papers and speaking at conferences.\n",
    "His favorite color is light blue.\n",
    "He likes pandas.\n",
    "\"\"\"\n",
    "response = llama(prompt, temperature=0.0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065d446f-2eef-4f34-858b-c5146e154cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the code again - the output should be identical\n",
    "response = llama(prompt, temperature=0.0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb83e70c-c629-470a-8013-cc63c56b5870",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Help me write a birthday card for my dear friend Andrew.\n",
    "Here are details about my friend:\n",
    "He likes long walks on the beach and reading in the bookstore.\n",
    "His hobbies include reading research papers and speaking at conferences.\n",
    "His favorite color is light blue.\n",
    "He likes pandas.\n",
    "\"\"\"\n",
    "response = llama(prompt, temperature=0.9)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6834c2db-16c7-46ed-8c79-9aac6041fe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the code again - the output should be different\n",
    "response = llama(prompt, temperature=0.9)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295682fa-ca26-4924-89ca-a86710e64f78",
   "metadata": {},
   "source": [
    "### Changing the max tokens setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede4dfa8-0d2c-42da-b588-701119bd136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Help me write a birthday card for my dear friend Andrew.\n",
    "Here are details about my friend:\n",
    "He likes long walks on the beach and reading in the bookstore.\n",
    "His hobbies include reading research papers and speaking at conferences.\n",
    "His favorite color is light blue.\n",
    "He likes pandas.\n",
    "\"\"\"\n",
    "response = llama(prompt,max_tokens=20)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5333a590-b54a-44f3-a627-1db0fe462315",
   "metadata": {},
   "source": [
    "The next cell reads in the text of the children's book *The Velveteen Rabbit* by Margery Williams, and stores it as a string named `text`. (Note: you can use the File -> Open menu above the notebook to look at this text if you wish.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0a6d79-8c32-4b56-869d-ec7b4b9e526c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"TheVelveteenRabbit.txt\", \"r\", encoding='utf=8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e31bf5-b323-4771-ae59-70b73e00ec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Give me a summary of the following text in 50 words:\\n\\n\n",
    "{text}\n",
    "\"\"\"\n",
    "response = llama(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f15ff3c-0068-4abe-8e6f-e3baf92dd31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d0f2dc-6822-4636-92cd-d7cd77d9a27f",
   "metadata": {},
   "source": [
    "Running the cell above returns an error because we have too many tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04efbc3e-e67f-4300-a46d-7bf3834077dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of input tokens (prompt + Velveteen Rabbit text) and output tokens\n",
    "3974 + 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ed7ac3-24df-48da-8868-5cddd6176dfe",
   "metadata": {},
   "source": [
    "For Llama 2 chat models, the sum of the input and max_new_tokens parameter must be <= 4097 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7e1c8d-b16c-41af-8c83-79ce559f860a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate tokens available for response after accounting for 3974 input tokens\n",
    "4097 - 3974"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93887be-c3ea-44d2-86fc-a4aa9e101a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set max_tokens to stay within limit on input + output tokens\n",
    "prompt = f\"\"\"\n",
    "Give me a summary of the following text in 50 words:\\n\\n\n",
    "{text}\n",
    "\"\"\"\n",
    "response = llama(prompt,\n",
    "                max_tokens=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e8b6db-e1ac-4480-bd86-d57334d6db57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccbe1ce-35f6-4bdb-888a-5232b3a23794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase max_tokens beyond limit on input + output tokens\n",
    "prompt = f\"\"\"\n",
    "Give me a summary of the following text in 50 words:\\n\\n\n",
    "{text}\n",
    "\"\"\"\n",
    "response = llama(prompt,\n",
    "                max_tokens=124)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e768b69e-ee6c-445f-b68b-a7480a76a019",
   "metadata": {},
   "source": [
    "### Asking a follow up question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b613eb4a-c922-4037-ad1e-ffb8cdea1c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Help me write a birthday card for my dear friend Andrew.\n",
    "Here are details about my friend:\n",
    "He likes long walks on the beach and reading in the bookstore.\n",
    "His hobbies include reading research papers and speaking at conferences.\n",
    "His favorite color is light blue.\n",
    "He likes pandas.\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4333b1d1-e4fe-436a-a3af-d23eed09b866",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_2 = \"\"\"\n",
    "Oh, he also likes teaching. Can you rewrite it to include that?\n",
    "\"\"\"\n",
    "response_2 = llama(prompt_2)\n",
    "print(response_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f8355e-2da8-411d-8bf1-c0b545ddc4da",
   "metadata": {},
   "source": [
    "### (Optional): Using Llama-2 on your own computer!\n",
    "- Llama-2 is free to download on your own machine!\n",
    "- One way to install and use llama on your computer is to go to https://ollama.com/ and download app. It will be like installing a regular application.\n",
    "- To use llama-2, the full instructions are here: https://ollama.com/library/llama2\n",
    "\n",
    "#### Here's an quick summary of how to get started:\n",
    "  - Follow the installation instructions (for Windows, Mac or Linux).\n",
    "  - Open the command line interface (CLI) and type `ollama run llama2`.\n",
    "  - The first time you do this, it will take some time to download the llama-2 model. After that, you'll see \n",
    "> `>>> Send a message (/? for help)`\n",
    "\n",
    "- You can type your prompt and the llama-2 model on your computer will give you a response!\n",
    "- To exit, type `/bye`.\n",
    "- For a list of other commands, type `/?`.\n",
    "\n",
    "![](ollama_example.png \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71958a71-89d9-4ccb-88e2-97fa173bb09e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
