{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13cc6594-88b5-4728-96c7-d5563b07be28",
   "metadata": {},
   "source": [
    "# Lab 7: Llama Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20efa5ef-5484-4b9d-b31d-2166cc35f876",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ‚è≥ <b>Note <code>(Kernel Starting)</code>:</b> This notebook takes about 30 seconds to be ready to use. You may start and watch the video while you wait.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303897a5-2dbc-4d0c-af34-a35769f8b9b1",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd945c34-f7d4-41ef-8c57-2150f5c8b7cb",
   "metadata": {
    "height": 65
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "_ = load_dotenv() #loads 'TOGETHER_API_KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125d6f57-cf6a-4b1a-9187-83ea5617f09a",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "#!pip install llama-stack==0.0.36 llama-stack-client==0.0.35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e318db-d0c3-4d6c-b3d6-f11832574e93",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6ff; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\"> üíª &nbsp; <b>Access <code>requirements.txt</code> and <code>utils.py</code> files:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em>. For more help, please see the <em>\"Appendix - Tips and Help\"</em> Lesson.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456331d9-0be8-45b8-bdf6-c55e9772075e",
   "metadata": {
    "height": 31
   },
   "outputs": [],
   "source": [
    "!llama stack build --list-templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5632e7d6-b4ab-4d97-8ac5-3e962207a9fb",
   "metadata": {
    "height": 31
   },
   "outputs": [],
   "source": [
    "!llama stack list-apis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b2c493-8cb6-4011-ab2a-07bc64691058",
   "metadata": {},
   "source": [
    "#  Llama Stack Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed253e6-68ca-4f99-ae2c-fd4387c15401",
   "metadata": {
    "height": 591
   },
   "outputs": [],
   "source": [
    "LLAMA_STACK_API_TOGETHER_URL=\"https://llama-stack.together.ai\"\n",
    "LLAMA31_8B_INSTRUCT = \"Llama3.1-8B-Instruct\"\n",
    "\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client.lib.inference.event_logger import EventLogger\n",
    "from llama_stack_client.types import UserMessage\n",
    "\n",
    "async def run_main():\n",
    "    client = LlamaStackClient(\n",
    "        base_url=LLAMA_STACK_API_TOGETHER_URL,\n",
    "    )\n",
    "\n",
    "    iterator = client.inference.chat_completion(\n",
    "        messages=[\n",
    "            UserMessage(\n",
    "                content=\"Who wrote the book Innovator's Dilemma? How about Charlotte's Web?\",\n",
    "                role=\"user\",\n",
    "            ),\n",
    "\n",
    "            UserMessage(\n",
    "                content=\"which book was published first?\",\n",
    "                role=\"user\",\n",
    "            ),\n",
    "        ],\n",
    "        model=LLAMA31_8B_INSTRUCT,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    async for log in EventLogger().log(iterator):\n",
    "        log.print()\n",
    "        #print(\"?\")\n",
    "\n",
    "await run_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b9dca1-3536-4637-a46a-a80d2776adad",
   "metadata": {
    "id": "JSOezWytd0pA"
   },
   "source": [
    "# Llama Stack Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1457c6d-dd5c-46ad-a565-569067a4f2cd",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import List, Optional, Dict\n",
    "\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "\n",
    "from llama_stack_client.types import SamplingParams, UserMessage\n",
    "from llama_stack_client.types.agent_create_params import AgentConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98e2274-ef41-41b8-aabb-aa97ef847405",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 591,
    "id": "NGbay0VcWFe4",
    "outputId": "8c79cfc2-9b1b-4d55-9347-307e4ec38e43"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.client = LlamaStackClient(\n",
    "            base_url=LLAMA_STACK_API_TOGETHER_URL,\n",
    "        )\n",
    "\n",
    "    def create_agent(self, agent_config: AgentConfig):\n",
    "        agent = self.client.agents.create(\n",
    "            agent_config=agent_config,\n",
    "        )\n",
    "        self.agent_id = agent.agent_id\n",
    "        session = self.client.agents.sessions.create(\n",
    "            agent_id=agent.agent_id,\n",
    "            session_name=\"example_session\",\n",
    "        )\n",
    "        self.session_id = session.session_id\n",
    "\n",
    "    async def execute_turn(self, content: str):\n",
    "        response = self.client.agents.turns.create(\n",
    "            agent_id=self.agent_id,\n",
    "            session_id=self.session_id,\n",
    "            messages=[\n",
    "                UserMessage(content=content, role=\"user\"),\n",
    "            ],\n",
    "            stream=True,\n",
    "        )\n",
    "\n",
    "        for chunk in response:\n",
    "            if chunk.event.payload.event_type != \"turn_complete\":\n",
    "                yield chunk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0dd18f-316a-43eb-87f4-5dec8568f65a",
   "metadata": {
    "height": 421
   },
   "outputs": [],
   "source": [
    "async def run_main():\n",
    "    agent_config = AgentConfig(\n",
    "        model=LLAMA31_8B_INSTRUCT,\n",
    "        instructions=\"You are a helpful assistant\",\n",
    "        enable_session_persistence=False,\n",
    "    )\n",
    "\n",
    "    agent = Agent()\n",
    "    agent.create_agent(agent_config)\n",
    "\n",
    "    prompts = [\n",
    "        \"Who wrote the book Charlotte's Web?\",\n",
    "        \"Three best quotes?\",\n",
    "    ]\n",
    "\n",
    "    for prompt in prompts:\n",
    "        print(f\"User> {prompt}\")\n",
    "        response = agent.execute_turn(content=prompt)\n",
    "        async for log in EventLogger().log(response):\n",
    "            if log is not None:\n",
    "                log.print()\n",
    "\n",
    "await run_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1871348-8265-4a8a-8add-d682fb9c7605",
   "metadata": {
    "id": "IPVNKOxYwvKH"
   },
   "source": [
    "# Llama Stack with Llama 3.2 vision model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879128d7-3b7e-411a-a099-0345a023d14d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "height": 184,
    "id": "wxNRgPhg8o9X",
    "outputId": "7854bed8-ed54-4ecf-abf2-d1b822c3330b"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_image(path):\n",
    "  img = Image.open(path)\n",
    "  plt.imshow(img)\n",
    "  plt.axis('off')\n",
    "  plt.show()\n",
    "\n",
    "display_image(\"./content/Llama_Repo.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a8122f-fd03-4cc5-be55-9ecc98d9a5ef",
   "metadata": {
    "height": 217
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client.types import agent_create_params\n",
    "\n",
    "LLAMA32_11B_INSTRUCT = \"Llama3.2-11B-Vision-Instruct\"\n",
    "\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as img:\n",
    "    return base64.b64encode(img.read()).decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0db8c2e-8162-4b31-94ae-90012560ce58",
   "metadata": {
    "height": 744
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.client = LlamaStackClient(\n",
    "            base_url=LLAMA_STACK_API_TOGETHER_URL,\n",
    "        )\n",
    "\n",
    "    def create_agent(self, agent_config: AgentConfig):\n",
    "        agent = self.client.agents.create(\n",
    "            agent_config=agent_config,\n",
    "        )\n",
    "        self.agent_id = agent.agent_id\n",
    "        session = self.client.agents.sessions.create(\n",
    "            agent_id=agent.agent_id,\n",
    "            session_name=\"example_session\",\n",
    "        )\n",
    "        self.session_id = session.session_id\n",
    "\n",
    "    async def execute_turn(self, prompt: str, image_path: str):\n",
    "        base64_image = encode_image(image_path)\n",
    "\n",
    "        messages = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "              {\n",
    "                \"image\": {\n",
    "                  \"uri\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                }\n",
    "              },\n",
    "              prompt,\n",
    "            ]\n",
    "        }]\n",
    "\n",
    "        response = self.client.agents.turns.create(\n",
    "            agent_id=self.agent_id,\n",
    "            session_id=self.session_id,\n",
    "            messages = messages,\n",
    "            stream=True,\n",
    "        )\n",
    "\n",
    "        for chunk in response:\n",
    "            if chunk.event.payload.event_type != \"turn_complete\":\n",
    "                yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b22c80-3fb0-4989-806d-68371b3cdad4",
   "metadata": {
    "height": 319
   },
   "outputs": [],
   "source": [
    "async def run_main(image_path, prompt):\n",
    "    agent_config = AgentConfig(\n",
    "        model=LLAMA32_11B_INSTRUCT,\n",
    "        instructions=\"You are a helpful assistant\",\n",
    "        enable_session_persistence=False,\n",
    "    )\n",
    "\n",
    "    agent = Agent()\n",
    "    agent.create_agent(agent_config)\n",
    "\n",
    "    print(f\"User> {prompt}\")\n",
    "    response = agent.execute_turn(prompt=prompt, image_path=image_path)\n",
    "    async for log in EventLogger().log(response):\n",
    "        if log is not None:\n",
    "            log.print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0902e4d1-6d8d-4407-9edc-a41dfd910f82",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "await run_main(\"./content/Llama_Repo.jpeg\",\n",
    "         \"How many different colors are those llamas?\\\n",
    "         What are those colors?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
