{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40855c11",
   "metadata": {},
   "source": [
    "# Lesson 5: Large Vision-Language Models (LVLMs)\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3995fdac-3453-4139-a2b3-5c9f60d14d4d",
   "metadata": {
    "height": 167
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from utils import encode_image\n",
    "from utils import (\n",
    "    prediction_guard_llava_conv, \n",
    "    lvlm_inference_with_conversation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c651071d",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a015b5-2dd5-475f-be33-039c81319a7f",
   "metadata": {
    "height": 455
   },
   "outputs": [],
   "source": [
    "# Image from COCO dataset\n",
    "url1 = (\n",
    "    'https://farm4.staticflickr.com/'\n",
    "    '3300/3497460990_11dfb95dd1_z.jpg'\n",
    ")\n",
    "img1_metadata = {\n",
    "    \"link\": url1,\n",
    "    \"transcript\": (\n",
    "        \"Wow, this trick is amazing!\"\n",
    "    ),        \n",
    "    \"path_to_file\": \n",
    "        \"./shared_data/skateboard.jpg\"\n",
    "}\n",
    "\n",
    "# an image and transcript extracted in Lesson 3\n",
    "img2_metadata = {\n",
    "    \"transcript\": (\n",
    "        \"As I look back on the the mission that \"\n",
    "        \"we've had here on the International Space Station, \"\n",
    "        \"I'm proud to have been a part of much of the \"\n",
    "        \"science activities that happened over the last two months.\"\n",
    "    ),\n",
    "    \"path_to_file\": \n",
    "        \"./shared_data/videos/video1/extracted_frame/frame_1.jpg\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c75b7d2-dd94-4505-ac90-e976932d9cf7",
   "metadata": {
    "height": 336
   },
   "outputs": [],
   "source": [
    "# another image an transcript extracted in Lesson 3\n",
    "img3_metadata = {\n",
    "    \"transcript\": (\n",
    "        \"the science activities that happened over the last \"\n",
    "        \"two months. The view is always amazing I didn't think \"\n",
    "        \"I would do another spacewalk and to now have the chance \"\n",
    "        \"to have done four more was just icing on the cake for a\"\n",
    "    ),\n",
    "    \"path_to_file\": \n",
    "        \"./shared_data/videos/video1/extracted_frame/frame_5.jpg\"\n",
    "}\n",
    "\n",
    "# download image 1\n",
    "if not Path(img1_metadata['path_to_file']).exists():\n",
    "    _ = urlretrieve(\n",
    "            img1_metadata['link'], \n",
    "            img1_metadata['path_to_file']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755b992b",
   "metadata": {},
   "source": [
    "## LLaVA\n",
    "\n",
    "- [LLaVA](https://llava-vl.github.io/) (Large Language-and-Vision Assistant), an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.\n",
    "- LLaVA doesn't just see images but understands them, reads the text embedded in them, and reasons about their contextâ€”all while conversing with you in a way that feels almost natural.\n",
    "- LLaVA is adept at tasks like explaining a complex chart, recognizing and reading text in photos, or identifying intricate details in high-resolution images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c99bc1",
   "metadata": {},
   "source": [
    "\n",
    "<p style=\"background-color:#fff1d7; padding:15px; \"> <b>Note:</b>\n",
    "<br>\n",
    "* The first user's message of a conversation must include a text prompt and a base64-encoded image.\n",
    "<br>  \n",
    "* The follow-up user's or assistant's messages of a conversation must include a text prompt only.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702d84eb",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "We have provided a dataclass `prediction_guard_llava_conv` and a helper function `lvlm_inference_with_conversation` in `utils.py` to support calling chat completion endpoint of PredictionGuard with a conversation. You can view utils.py using `file`->`open`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9771fc92",
   "metadata": {},
   "source": [
    "## Few Use Cases with Chat Completion with LLaVA\n",
    "### Image Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c2ea12-96fd-4383-a73a-9c90ec1ff59e",
   "metadata": {
    "height": 200
   },
   "outputs": [],
   "source": [
    "# prepare the prompt and image\n",
    "prompt = \"Please describe the image in detail\"\n",
    "image_path = img2_metadata['path_to_file']\n",
    "b64_img = encode_image(image_path)\n",
    "\n",
    "# prepare conversation\n",
    "img_captioning_conv = prediction_guard_llava_conv.copy()\n",
    "img_captioning_conv.append_message('user', [prompt, b64_img])\n",
    "# call chat completion endpoint from prediction guard\n",
    "caption = lvlm_inference_with_conversation(img_captioning_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fccaf1-0893-4bb9-822f-611534f7aa58",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# display caption and image\n",
    "display(Image.open(image_path))\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f30e72c",
   "metadata": {},
   "source": [
    "### QnA on Visual Cues on Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef41e88-b859-4107-a453-93c1b8f4be50",
   "metadata": {
    "height": 200
   },
   "outputs": [],
   "source": [
    "# prepare the prompt and image\n",
    "prompt = \"What is likely going to happen next?\"\n",
    "image_path = img1_metadata['path_to_file']\n",
    "b64_img = encode_image(image_path)\n",
    "\n",
    "# prepare conversation\n",
    "qna_visual_cues_conv = prediction_guard_llava_conv.copy()\n",
    "qna_visual_cues_conv.append_message('user', [prompt, b64_img])\n",
    "# call chat completion endpoint from prediction guard\n",
    "answer = lvlm_inference_with_conversation(qna_visual_cues_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd79d60b-7577-4559-bf29-d1b4e765ed32",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# display image and answer\n",
    "display(Image.open(image_path))\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ebb018",
   "metadata": {},
   "source": [
    "### QnA on Textual Cues on Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0e1fa2-4243-4af7-ab44-1d5b92343a53",
   "metadata": {
    "height": 268
   },
   "outputs": [],
   "source": [
    "# prepare the prompt and image\n",
    "prompt = 'What is the name of one of the astronauts?'\n",
    "image_path = img2_metadata['path_to_file']\n",
    "b64_img = encode_image(image_path)\n",
    "\n",
    "# prepare conversation\n",
    "qna_textual_cues_conv = prediction_guard_llava_conv.copy()\n",
    "qna_textual_cues_conv.append_message('user', [prompt, b64_img])\n",
    "# call chat completion endpoint from prediction guard\n",
    "answer = lvlm_inference_with_conversation(qna_textual_cues_conv)\n",
    "\n",
    "# display image and answer\n",
    "display(Image.open(image_path))\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcaff8c",
   "metadata": {},
   "source": [
    "### QnA on Caption/Transcript Associated with Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7fd907-79b0-4911-8e22-8f4dbcfed6a9",
   "metadata": {
    "height": 421
   },
   "outputs": [],
   "source": [
    "# prepare the prompt and image\n",
    "# include the transcript of the image in the prompt\n",
    "prompt_template = (\n",
    "    \"The transcript associated with the image is '{transcript}'. \"\n",
    "    \"What do the astronauts feel about their work?\"\n",
    ")\n",
    "prompt = prompt_template.format(\n",
    "    transcript=img2_metadata[\"transcript\"]\n",
    ")\n",
    "image_path = img2_metadata['path_to_file']\n",
    "b64_img = encode_image(image_path)\n",
    "\n",
    "# prepare conversation\n",
    "qna_transcript_conv = prediction_guard_llava_conv.copy()\n",
    "qna_transcript_conv.append_message('user', [prompt, b64_img])\n",
    "# call chat completion endpoint from prediction guard \n",
    "#  with temperature = 0.95 and top_k =2 \n",
    "#  to reduce randomness in LLaVA's response \n",
    "answer = lvlm_inference_with_conversation(\n",
    "    qna_transcript_conv, \n",
    "    temperature=0.95, \n",
    "    top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f12c116-cf7e-4aad-a0b8-3b2d8eb5ee55",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "# display image and answer\n",
    "display(Image.open(image_path))\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(\"Answer:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4090bbc5",
   "metadata": {},
   "source": [
    "### Multi-Turn QnA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce1a207-fbc1-4321-91ba-9691d4df59f3",
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": [
    "# extend conversation with the response of LVLM\n",
    "qna_transcript_conv.append_message('assistant', [answer])\n",
    "\n",
    "# extend conversation with a follow-up query\n",
    "follow_up_query = \"Where did the astronauts return from?\"\n",
    "qna_transcript_conv.append_message('user', [follow_up_query])\n",
    "\n",
    "# call chat completion endpoint from prediction guard\n",
    "follow_up_ans = lvlm_inference_with_conversation(qna_transcript_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfe0577-8be0-4e68-aa06-2f5b2d99cdae",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# display answer to the second turn\n",
    "print(\"Answer to the follow-up query:\")\n",
    "print(follow_up_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b9b514-335d-4cc6-8fc9-b6c9f67ef636",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
