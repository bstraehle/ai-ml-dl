{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bc9bbe5",
   "metadata": {},
   "source": [
    "# Lesson 6: Multimodal RAG with Multimodal Langchain\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fd4442-4b7b-45ec-a16d-45112e1650e6",
   "metadata": {
    "height": 268
   },
   "outputs": [],
   "source": [
    "import lancedb\n",
    "from utils import load_json_file\n",
    "from mm_rag.embeddings.bridgetower_embeddings import (\n",
    "    BridgeTowerEmbeddings\n",
    ")\n",
    "from mm_rag.vectorstores.multimodal_lancedb import MultimodalLanceDB\n",
    "from mm_rag.MLM.client import PredictionGuardClient\n",
    "from mm_rag.MLM.lvlm import LVLM\n",
    "from PIL import Image\n",
    "from langchain_core.runnables import (\n",
    "    RunnableParallel, \n",
    "    RunnablePassthrough, \n",
    "    RunnableLambda\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1772071",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "#### Setup LanceDB vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15debeb1",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff1d7; padding:15px; \"> <b>Note:</b>\n",
    "We'll use the Youtube video \n",
    "<br>\n",
    "<a href=\"https://www.youtube.com/watch?v=7Hcg-rLYwdM\">\n",
    "https://www.youtube.com/watch?v=7Hcg-rLYwdM</a> \n",
    "<br>\n",
    "preprocessed in Lesson 3 and ingested to LanceDB in Lesson 4.\n",
    "<br>\n",
    "If you haven't created vectorstore `test_tbl` in Lesson 4, \n",
    "<br>\n",
    "please update <code>TBL_NAME=\"demo_tbl\"</code> to use the pre-populated data.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7b7806-b5ee-4ba4-a2ce-422cb44ac50e",
   "metadata": {
    "height": 200
   },
   "outputs": [],
   "source": [
    "# declare host file\n",
    "LANCEDB_HOST_FILE = \"./shared_data/.lancedb\"\n",
    "\n",
    "# declare table name\n",
    "TBL_NAME = \"test_tbl\"\n",
    "\n",
    "# if you haven't practice Lesson 3 and 4, \n",
    "#   change 'test_tbl' to 'demo_tbl' to use pre-populated data\n",
    "#   by uncomment the following line\n",
    "#TBL_NAME = \"demo_tbl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d31236e",
   "metadata": {},
   "source": [
    "### Retrieval Module\n",
    "#### Initialize Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33744c7e-0925-4f2a-851a-8ff1bbd043b2",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# initialize an BridgeTower embedder \n",
    "embedder = BridgeTowerEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99411d0",
   "metadata": {},
   "source": [
    "#### Create Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d651f1e-f3ea-4f06-a0bd-99126cd7de13",
   "metadata": {
    "height": 251
   },
   "outputs": [],
   "source": [
    "## Creating a LanceDB vector store \n",
    "vectorstore = MultimodalLanceDB(\n",
    "    uri=LANCEDB_HOST_FILE, \n",
    "    embedding=embedder, \n",
    "    table_name=TBL_NAME\n",
    ")\n",
    "\n",
    "### creating a retriever for the vector store\n",
    "### with search_type=\"similarity\" and search_kwargs={\"k\": 1} \n",
    "retriever_module = vectorstore.as_retriever(\n",
    "    search_type='similarity', \n",
    "    search_kwargs={\"k\": 1}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b458d619",
   "metadata": {},
   "source": [
    "#### Invoke Retrieval with User Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe2cbd0-9593-47b4-a914-2b2a0d9b30fb",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "# Invoke the retrieval for a query\n",
    "query = \"What do the astronauts feel about their work?\"\n",
    "retrieved_video_segments = retriever_module.invoke(query)\n",
    "# get the first retrieved video segment\n",
    "retrieved_video_segment = retrieved_video_segments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4c6038-49b7-44bf-ac40-6f8374b610bc",
   "metadata": {
    "height": 336
   },
   "outputs": [],
   "source": [
    "# get all metadata of the retrieved video segment\n",
    "retrieved_metadata = retrieved_video_segment.metadata['metadata']\n",
    "\n",
    "# get the extracted frame\n",
    "frame_path = retrieved_metadata['extracted_frame_path']\n",
    "# get the corresponding transcript\n",
    "transcript = retrieved_metadata['transcript']\n",
    "# get the path to video where the frame was extracted\n",
    "video_path = retrieved_metadata['video_path']\n",
    "# get the time stamp when the frame was extracted\n",
    "timestamp = retrieved_metadata['mid_time_ms']\n",
    "\n",
    "# display\n",
    "print(f\"Transcript:\\n{transcript}\\n\")\n",
    "print(f\"Path to extracted frame: {frame_path}\")\n",
    "print(f\"Path to video: {video_path}\")\n",
    "print(f\"Timestamp in ms when the frame was extracted: {timestamp}\")\n",
    "display(Image.open(frame_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ec3e85",
   "metadata": {},
   "source": [
    "### LVLM Inference Module\n",
    "\n",
    "#### Initialize Client and LVLM for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fae7bd7-a4fa-4395-9071-1f4b00e6ea96",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "# initialize a client as PredictionGuardClient\n",
    "client = PredictionGuardClient()\n",
    "# initialize LVLM with the given client\n",
    "lvlm_inference_module = LVLM(client=client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3286d225",
   "metadata": {},
   "source": [
    "#### Invoke LVLM Inference with User Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e99fca-56d4-40f8-aca1-4df2c351f605",
   "metadata": {
    "height": 217
   },
   "outputs": [],
   "source": [
    "# This new query is the augmentation of the previous query\n",
    "# with the transcript retrieved above. \n",
    "augmented_query_template = (\n",
    "    \"The transcript associated with the image is '{transcript}'. \"\n",
    "    \"{previous_query}\"\n",
    ")\n",
    "augmented_query = augmented_query_template.format(\n",
    "    transcript=transcript,\n",
    "    previous_query=query,\n",
    ")\n",
    "print(f\"Augmented query is:\\n{augmented_query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04136236-fbdf-4c53-aa4a-7a5e0e3d5de4",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "# we use the augmented query and the retrieved path-to-image\n",
    "# as the input to LVLM inference module\n",
    "input = {'prompt':augmented_query, 'image': frame_path}\n",
    "response = lvlm_inference_module.invoke(input)\n",
    "\n",
    "# display the response\n",
    "print('LVLM Response:')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650be83a",
   "metadata": {},
   "source": [
    "### Prompt Processing Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd4ec3d-d969-4d88-8320-0dc224befdf4",
   "metadata": {
    "height": 557
   },
   "outputs": [],
   "source": [
    "def prompt_processing(input):\n",
    "    # get the retrieved results and user's query\n",
    "    retrieved_results = input['retrieved_results']\n",
    "    user_query = input['user_query']\n",
    "    \n",
    "    # get the first retrieved result by default\n",
    "    retrieved_result = retrieved_results[0]\n",
    "    prompt_template = (\n",
    "      \"The transcript associated with the image is '{transcript}'. \"\n",
    "      \"{user_query}\"\n",
    "    )\n",
    "    \n",
    "    # get all metadata of the retrieved video segment\n",
    "    retrieved_metadata = retrieved_result.metadata['metadata']\n",
    "\n",
    "    # get the corresponding transcript\n",
    "    transcript = retrieved_metadata['transcript']\n",
    "    # get the extracted frame\n",
    "    frame_path = retrieved_metadata['extracted_frame_path']\n",
    "    \n",
    "    return {\n",
    "        'prompt': prompt_template.format(\n",
    "            transcript=transcript, \n",
    "            user_query=user_query\n",
    "        ),\n",
    "        'image' : frame_path\n",
    "    }\n",
    "    \n",
    "# initialize prompt processing module \n",
    "# as a Langchain RunnableLambda of function prompt_processing\n",
    "prompt_processing_module = RunnableLambda(prompt_processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f31c2d",
   "metadata": {},
   "source": [
    "#### Invoke Prompt Processing Module with Retrieved Results and User Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0181c7-53c4-49d7-80bb-50edcbde6900",
   "metadata": {
    "height": 200
   },
   "outputs": [],
   "source": [
    "# We use the user query and the retrieved results above\n",
    "input_to_lvlm = prompt_processing_module.invoke(\n",
    "    {\n",
    "        'retrieved_results': retrieved_video_segments, \n",
    "        'user_query': query\n",
    "    })\n",
    "\n",
    "# display output of prompt processing module \n",
    "#  which is the input to LVLM Inference module\n",
    "print(input_to_lvlm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f32f71f",
   "metadata": {},
   "source": [
    "### Multimodal RAG\n",
    "\n",
    "#### Define Multimodal RAG System as a Chain in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6287088-237b-42da-92e4-77a0b0257fb0",
   "metadata": {},
   "source": [
    "We are going to make use of the followings from `Langchain`:\n",
    "- The `RunnableParallel` primitive is essentially a dict whose values are runnables (or things that can be coerced to runnables, like functions). It runs all of its values in parallel, and each value is called with the overall input of the RunnableParallel. The final return value is a dict with the results of each value under its appropriate key.\n",
    "- The `RunnablePassthrough` on its own allows you to pass inputs unchanged. This typically is used in conjuction with RunnableParallel to pass data through to a new key in the map.\n",
    "- The `RunnableLambda` converts a python function into a Runnable. \n",
    "Wrapping a function in a RunnableLambda makes the function usable within either a sync or async context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccff79b-500f-4685-9ee8-6b3801f5a145",
   "metadata": {
    "height": 200
   },
   "outputs": [],
   "source": [
    "# combine all the modules into a chain \n",
    "# to create Multimodal RAG system\n",
    "mm_rag_chain = (\n",
    "    RunnableParallel({\n",
    "        \"retrieved_results\": retriever_module , \n",
    "        \"user_query\": RunnablePassthrough()\n",
    "    }) \n",
    "    | prompt_processing_module\n",
    "    | lvlm_inference_module\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b5cf5e",
   "metadata": {},
   "source": [
    "#### Invoke the Multimodal RAG System with a Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e008a40-cce9-4ae7-8e54-04064545bf77",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "# invoke the Multimodal RAG system with a query\n",
    "query1 = \"What do the astronauts feel about their work?\"\n",
    "final_text_response1 = mm_rag_chain.invoke(query1)\n",
    "# display\n",
    "print(f\"USER Query: {query1}\")\n",
    "print(f\"MM-RAG Response: {final_text_response1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0563f5a5-a7a3-46ae-9415-e0c56efd899d",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "# let's try another query\n",
    "query2 = \"What is the name of one of the astronauts?\"\n",
    "final_text_response2 = mm_rag_chain.invoke(query2)\n",
    "# display\n",
    "print(f\"USER Query: {query2}\")\n",
    "print(f\"MM-RAG Response: {final_text_response2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4a9834",
   "metadata": {},
   "source": [
    "#### Multimodal RAG System Showing Retrieved Image/Frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9658c162-95ff-430d-b2c9-5f0c2c94cf22",
   "metadata": {
    "height": 234
   },
   "outputs": [],
   "source": [
    "# the output of this new chain will be a dictionary\n",
    "mm_rag_chain_with_retrieved_image = (\n",
    "    RunnableParallel({\n",
    "        \"retrieved_results\": retriever_module , \n",
    "        \"user_query\": RunnablePassthrough()\n",
    "    }) \n",
    "    | prompt_processing_module\n",
    "    | RunnableParallel({\n",
    "        'final_text_output': lvlm_inference_module, \n",
    "        'input_to_lvlm' : RunnablePassthrough()\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89db54ac-ad84-48c6-a6e2-62efe5fc504b",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "# let's try again with query2\n",
    "response3 = mm_rag_chain_with_retrieved_image.invoke(query2)\n",
    "# display\n",
    "print(\"Type of output of mm_rag_chain_with_retrieved_image is:\")\n",
    "print(type(response3))\n",
    "print(f\"Keys of the dict are {response3.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85acfa1-53a0-4223-a88b-4544c1ff89cc",
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": [
    "# We now extract final text response and path to extracted frame\n",
    "final_text_response3 = response3['final_text_output']\n",
    "path_to_extracted_frame = response3['input_to_lvlm']['image']\n",
    "\n",
    "# display\n",
    "print(f\"USER Query: {query2}\")\n",
    "print(f\"MM-RAG Response: {final_text_response3}\")\n",
    "print(\"Retrieved frame:\")\n",
    "display(Image.open(path_to_extracted_frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1895ea-a893-4e85-9657-77af20102dad",
   "metadata": {
    "height": 251
   },
   "outputs": [],
   "source": [
    "# let's try again with another query\n",
    "query4 = \"an astronaut's spacewalk\"\n",
    "response4 = mm_rag_chain_with_retrieved_image.invoke(query4)\n",
    "# extract results\n",
    "final_text_response4 = response4['final_text_output']\n",
    "path_to_extracted_frame4 = response4['input_to_lvlm']['image']\n",
    "# display\n",
    "print(f\"USER Query: {query4}\")\n",
    "print()\n",
    "print(f\"MM-RAG Response: {final_text_response4}\")\n",
    "print()\n",
    "print(\"Retrieved frame:\")\n",
    "display(Image.open(path_to_extracted_frame4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567c2fb2-ba7d-4152-9edc-c34c279f6917",
   "metadata": {
    "height": 302
   },
   "outputs": [],
   "source": [
    "# We would like an astronaut's spacewalk with the earth view behind\n",
    "query5 = (\n",
    "    \"Describe the image of an astronaut's spacewalk \"\n",
    "    \"with an amazing view of the earth from space behind\"\n",
    ")\n",
    "response5 = mm_rag_chain_with_retrieved_image.invoke(query5)\n",
    "# extract results\n",
    "final_text_response5 = response5['final_text_output']\n",
    "path_to_extracted_frame5 = response5['input_to_lvlm']['image']\n",
    "# display\n",
    "print(f\"USER Query: {query5}\")\n",
    "print()\n",
    "print(f\"MM-RAG Response: {final_text_response5}\")\n",
    "print()\n",
    "print(\"Retrieved Frame:\")\n",
    "display(Image.open(path_to_extracted_frame5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e92f991-936f-45c5-bd81-114d1946bddd",
   "metadata": {
    "height": 302
   },
   "outputs": [],
   "source": [
    "# Slightly change the query5\n",
    "query6 = (\n",
    "    \"An astronaut's spacewalk with \"\n",
    "    \"an amazing view of the earth from space behind\"\n",
    ")\n",
    "response6 = mm_rag_chain_with_retrieved_image.invoke(query6)\n",
    "# extract results\n",
    "final_text_response6 = response6['final_text_output']\n",
    "path_to_extracted_frame6 = response6['input_to_lvlm']['image']\n",
    "# display\n",
    "print(f\"USER Query: {query6}\")\n",
    "print()\n",
    "print(f\"MM-RAG Response: {final_text_response6}\")\n",
    "print()\n",
    "print(\"Retrieved Frame:\")\n",
    "display(Image.open(path_to_extracted_frame6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6efbce5",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff1d7; padding:15px; \"> <b>Note:</b> \n",
    "Slightly changing the query may lead to different retrieved results \n",
    "and thus different final response.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cf65c1-eb67-44ad-b97c-ddc43ede126a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4211265-761e-4684-9ef5-213912e71ad0",
   "metadata": {},
   "source": [
    "#### To access latest Intel&reg; Gaudi&reg; 3 AI Accelerator and large-scale AI clusters please visit [cloud.intel.com](https://cloud.intel.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7112ef4-f91e-49b2-aedb-8da73d6b3ccd",
   "metadata": {},
   "source": [
    "![Intel Tiber Developer Cloud](../assets/ITDC.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6804ad31-9257-473c-a79e-90f49417b4f4",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
