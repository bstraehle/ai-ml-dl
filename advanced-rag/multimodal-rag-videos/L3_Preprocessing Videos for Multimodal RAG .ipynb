{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fdfe61a",
   "metadata": {},
   "source": [
    "# Lesson 3: Preprocessing Videos for Multimodal RAG \n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663198e8-2264-4175-ae2e-4e8f0048b046",
   "metadata": {
    "height": 184
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from os import path as osp\n",
    "import json\n",
    "import cv2\n",
    "import webvtt\n",
    "import whisper\n",
    "from moviepy.editor import VideoFileClip\n",
    "from PIL import Image\n",
    "import base64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c80dfba",
   "metadata": {},
   "source": [
    "### Download Video Corpuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e741f2-7b7b-4906-9222-a4c2a708e98f",
   "metadata": {
    "height": 200
   },
   "outputs": [],
   "source": [
    "from utils import download_video, get_transcript_vtt\n",
    "# first video's url\n",
    "vid1_url = \"https://www.youtube.com/watch?v=7Hcg-rLYwdM\"\n",
    "\n",
    "# download Youtube video to ./shared_data/videos/video1\n",
    "vid1_dir = \"./shared_data/videos/video1\"\n",
    "vid1_filepath = download_video(vid1_url, vid1_dir)\n",
    "\n",
    "# download Youtube video's subtitle to ./shared_data/videos/video1\n",
    "vid1_transcript_filepath = get_transcript_vtt(vid1_url, vid1_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc394ab-5469-4999-9ebf-b40289d75fc6",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "# show the paths to video1 and its transcription\n",
    "print(vid1_filepath)\n",
    "print(vid1_transcript_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eda924-3830-43de-a012-7940cd2ad5f1",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "!head -n15 {vid1_transcript_filepath}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1b98a7-ba3d-49a3-a618-b652fad264c0",
   "metadata": {
    "height": 285
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "# second video's url\n",
    "vid2_url=(\n",
    "    \"https://multimedia-commons.s3-us-west-2.amazonaws.com/\" \n",
    "    \"data/videos/mp4/010/a07/010a074acb1975c4d6d6e43c1faeb8.mp4\"\n",
    ")\n",
    "vid2_dir = \"./shared_data/videos/video2\"\n",
    "vid2_name = \"toddler_in_playground.mp4\"\n",
    "\n",
    "# create folder to which video2 will be downloaded \n",
    "Path(vid2_dir).mkdir(parents=True, exist_ok=True)\n",
    "vid2_filepath = urlretrieve(\n",
    "                        vid2_url, \n",
    "                        osp.join(vid2_dir, vid2_name)\n",
    "                    )[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45282029",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d864398-47f7-4085-869d-1411745b11f9",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from utils import str2time\n",
    "from utils import maintain_aspect_ratio_resize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230cce14",
   "metadata": {},
   "source": [
    "## 1. Video Corpus and Its Transcript Are Available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97f2f6e",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff1d7; padding:15px; \"> <b>Note:</b> \n",
    "    For each video segment, we will extract:\n",
    "<br>\n",
    "1. A frame right at the middle of the time frame of the video segment;\n",
    "<br>\n",
    "2. Its metadata including:\n",
    "<br>\n",
    "-<code>extracted_frame_path</code>: Path to the saved extracted-frame;\n",
    "<br>\n",
    "-<code>transcript</code>: Transcript of the extracted frame;\n",
    "<br>\n",
    "-<code>video_segment_id</code>: \n",
    "    The order of video segment from which the frame\n",
    "<br>\n",
    "was extracted;\n",
    "<br>\n",
    "-<code>video_path</code>: \n",
    "    Path to the video from which the frame was extracted; \n",
    "<br>\n",
    "This helps to retrieve the correct video when there are many ones\n",
    "<br>\n",
    " in your video corpus;\n",
    "<br>\n",
    "-<code>mid_time_ms</code>: Time stamp (in ms) of the extracted frame\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781e36e3-e69b-476a-82f4-cb4b1b14fbdf",
   "metadata": {
    "height": 1050
   },
   "outputs": [],
   "source": [
    "# function `extract_and_save_frames_and_metadata``:\n",
    "#   receives as input a video and its transcript\n",
    "#   does extracting and saving frames and their metadatas\n",
    "#   returns the extracted metadatas\n",
    "def extract_and_save_frames_and_metadata(\n",
    "        path_to_video, \n",
    "        path_to_transcript, \n",
    "        path_to_save_extracted_frames,\n",
    "        path_to_save_metadatas):\n",
    "    \n",
    "    # metadatas will store the metadata of all extracted frames\n",
    "    metadatas = []\n",
    "\n",
    "    # load video using cv2\n",
    "    video = cv2.VideoCapture(path_to_video)\n",
    "    # load transcript using webvtt\n",
    "    trans = webvtt.read(path_to_transcript)\n",
    "    \n",
    "    # iterate transcript file\n",
    "    # for each video segment specified in the transcript file\n",
    "    for idx, transcript in enumerate(trans):\n",
    "        # get the start time and end time in seconds\n",
    "        start_time_ms = str2time(transcript.start)\n",
    "        end_time_ms = str2time(transcript.end)\n",
    "        # get the time in ms exactly \n",
    "        # in the middle of start time and end time\n",
    "        mid_time_ms = (end_time_ms + start_time_ms) / 2\n",
    "        # get the transcript, remove the next-line symbol\n",
    "        text = transcript.text.replace(\"\\n\", ' ')\n",
    "        # get frame at the middle time\n",
    "        video.set(cv2.CAP_PROP_POS_MSEC, mid_time_ms)\n",
    "        success, frame = video.read()\n",
    "        if success:\n",
    "            # if the frame is extracted successfully, resize it\n",
    "            image = maintain_aspect_ratio_resize(frame, height=350)\n",
    "            # save frame as JPEG file\n",
    "            img_fname = f'frame_{idx}.jpg'\n",
    "            img_fpath = osp.join(\n",
    "                path_to_save_extracted_frames, img_fname\n",
    "            )\n",
    "            cv2.imwrite(img_fpath, image)\n",
    "\n",
    "            # prepare the metadata\n",
    "            metadata = {\n",
    "                'extracted_frame_path': img_fpath,\n",
    "                'transcript': text,\n",
    "                'video_segment_id': idx,\n",
    "                'video_path': path_to_video,\n",
    "                'mid_time_ms': mid_time_ms,\n",
    "            }\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "        else:\n",
    "            print(f\"ERROR! Cannot extract frame: idx = {idx}\")\n",
    "\n",
    "    # save metadata of all extracted frames\n",
    "    fn = osp.join(path_to_save_metadatas, 'metadatas.json')\n",
    "    with open(fn, 'w') as outfile:\n",
    "        json.dump(metadatas, outfile)\n",
    "    return metadatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16197ff7-afe2-411e-a962-8dcd99ab0828",
   "metadata": {
    "height": 302
   },
   "outputs": [],
   "source": [
    "# output paths to save extracted frames and their metadata \n",
    "extracted_frames_path = osp.join(vid1_dir, 'extracted_frame')\n",
    "metadatas_path = vid1_dir\n",
    "\n",
    "# create these output folders if not existing\n",
    "Path(extracted_frames_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(metadatas_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# call the function to extract frames and metadatas\n",
    "metadatas = extract_and_save_frames_and_metadata(\n",
    "                vid1_filepath, \n",
    "                vid1_transcript_filepath,\n",
    "                extracted_frames_path,\n",
    "                metadatas_path,\n",
    "            )\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f40c89-f9f5-48d9-b7bc-3c0b220640d1",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "metadatas[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4652833",
   "metadata": {},
   "source": [
    "## 2. Video Corpus without Available Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672006b3-3b79-4506-a0c5-e38116cd65a6",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "path_to_video_no_transcript = vid1_filepath\n",
    "\n",
    "# declare where to save .mp3 audio\n",
    "path_to_extracted_audio_file = os.path.join(vid1_dir, 'audio.mp3')\n",
    "\n",
    "# extract mp3 audio file from mp4 video video file\n",
    "clip = VideoFileClip(path_to_video_no_transcript)\n",
    "clip.audio.write_audiofile(path_to_extracted_audio_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fb61c7",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff1d7; padding:15px; \"> <b>Notes:</b>\n",
    "<br>\n",
    "- This process usually takes a long time, around 1-2 minutes.\n",
    "<br>\n",
    "- For better performance, depend on how much memory your system has, \n",
    "<br>\n",
    "you might want to try larger whisper models (e.g., large-v2) and\n",
    "<br>\n",
    "try setting <code>best_of=5</code>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37546a06-8973-436e-9e0a-8093e33f012a",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "model = whisper.load_model(\"small\")\n",
    "options = dict(task=\"translate\", best_of=1, language='en')\n",
    "results = model.transcribe(path_to_extracted_audio_file, **options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59534a9-fd4b-43e4-b864-a9b142882888",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "from utils import getSubs\n",
    "vtt = getSubs(results[\"segments\"], \"vtt\")\n",
    "\n",
    "# path to save generated transcript of video1\n",
    "path_to_generated_trans = osp.join(vid1_dir, 'generated_video1.vtt')\n",
    "# write transcription to file\n",
    "with open(path_to_generated_trans, 'w') as f:\n",
    "    f.write(vtt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63df8c69-6462-4b89-a9e4-984324a77f18",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "!head {path_to_generated_trans}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e26dd2d",
   "metadata": {},
   "source": [
    "## 3. Video Corpus without Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983072d4-f3ad-4b51-859c-439b021e3451",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "lvlm_prompt = \"Can you describe the image?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd7a4da",
   "metadata": {},
   "source": [
    "### LVLM Inference Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00b5653-67b3-4d7f-9bb0-d2b5059f4646",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "path_to_frame = osp.join(vid1_dir, \"extracted_frame\", \"frame_5.jpg\")\n",
    "frame = Image.open(path_to_frame)\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365f92ba-fd01-4b14-ac32-e08269d627cc",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "from utils import lvlm_inference, encode_image\n",
    "# need to encode this frame with base64 encoding \n",
    "#  as input image to function lvlm_inference\n",
    "# encode image to base64\n",
    "image = encode_image(path_to_frame)\n",
    "caption = lvlm_inference(lvlm_prompt, image)\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc7bbb5",
   "metadata": {},
   "source": [
    "### Extract Frames and Metadata for Videos Using LVLM Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cd0ad8-6dd8-4a7d-9c7e-cbc64e6abd48",
   "metadata": {
    "height": 1033
   },
   "outputs": [],
   "source": [
    "# function extract_and_save_frames_and_metadata_with_fps\n",
    "#   receives as input a video \n",
    "#   does extracting and saving frames and their metadatas\n",
    "#   returns the extracted metadatas\n",
    "def extract_and_save_frames_and_metadata_with_fps(\n",
    "        path_to_video,  \n",
    "        path_to_save_extracted_frames,\n",
    "        path_to_save_metadatas,\n",
    "        num_of_extracted_frames_per_second=1):\n",
    "    \n",
    "    # metadatas will store the metadata of all extracted frames\n",
    "    metadatas = []\n",
    "\n",
    "    # load video using cv2\n",
    "    video = cv2.VideoCapture(path_to_video)\n",
    "    \n",
    "    # Get the frames per second\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    # Get hop = the number of frames pass before a frame is extracted\n",
    "    hop = round(fps / num_of_extracted_frames_per_second) \n",
    "    curr_frame = 0\n",
    "    idx = -1\n",
    "    while(True):\n",
    "        # iterate all frames\n",
    "        ret, frame = video.read()\n",
    "        if not ret: \n",
    "            break\n",
    "        if curr_frame % hop == 0:\n",
    "            idx = idx + 1\n",
    "        \n",
    "            # if the frame is extracted successfully, resize it\n",
    "            image = maintain_aspect_ratio_resize(frame, height=350)\n",
    "            # save frame as JPEG file\n",
    "            img_fname = f'frame_{idx}.jpg'\n",
    "            img_fpath = osp.join(\n",
    "                            path_to_save_extracted_frames, \n",
    "                            img_fname\n",
    "                        )\n",
    "            cv2.imwrite(img_fpath, image)\n",
    "\n",
    "            # generate caption using lvlm_inference\n",
    "            b64_image = encode_image(img_fpath)\n",
    "            caption = lvlm_inference(lvlm_prompt, b64_image)\n",
    "                \n",
    "            # prepare the metadata\n",
    "            metadata = {\n",
    "                'extracted_frame_path': img_fpath,\n",
    "                'transcript': caption,\n",
    "                'video_segment_id': idx,\n",
    "                'video_path': path_to_video,\n",
    "            }\n",
    "            metadatas.append(metadata)\n",
    "        curr_frame += 1\n",
    "        \n",
    "    # save metadata of all extracted frames\n",
    "    metadatas_path = osp.join(path_to_save_metadatas,'metadatas.json')\n",
    "    with open(metadatas_path, 'w') as outfile:\n",
    "        json.dump(metadatas, outfile)\n",
    "    return metadatas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de91e1b",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff1d7; padding:15px; \"> <b>Note:</b>\n",
    "<br>\n",
    "* The following process usually takes around <b>1</b> minutes.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a789ee4b-29cc-4911-b815-5494a3e7ee17",
   "metadata": {
    "height": 285
   },
   "outputs": [],
   "source": [
    "# paths to save extracted frames and metadata (their transcripts)\n",
    "extracted_frames_path = osp.join(vid2_dir, 'extracted_frame')\n",
    "metadatas_path = vid2_dir\n",
    "\n",
    "# create these output folders if not existing\n",
    "Path(extracted_frames_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(metadatas_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# call the function to extract frames and metadatas\n",
    "metadatas = extract_and_save_frames_and_metadata_with_fps(\n",
    "                vid2_filepath, \n",
    "                extracted_frames_path,\n",
    "                metadatas_path,\n",
    "                num_of_extracted_frames_per_second=0.1\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff45f257-0dff-41c4-9332-831932632300",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "data = metadatas[1]\n",
    "caption = data['transcript']\n",
    "print(f'Generated caption is: \"{caption}\"')\n",
    "frame = Image.open(data['extracted_frame_path'])\n",
    "display(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c6c04d",
   "metadata": {},
   "source": [
    "## Try experimenting on your own!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248879a6",
   "metadata": {},
   "source": [
    "### Notes on running whisper outside of this classroom\n",
    "To install `whisper`:\n",
    "```bash\n",
    "    !pip install git+https://github.com/openai/whisper.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13626a00",
   "metadata": {},
   "source": [
    "If calling `whisper` model throws an error about `ffmpeg`, you might want to use a FFmpeg static build in https://johnvansickle.com/ffmpeg/ (recommended in https://ffmpeg.org/download.html). \n",
    "\n",
    "See the provided bash script `./prepare_ffmpeg.sh` as example (Go to `File` and click `Open`).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
