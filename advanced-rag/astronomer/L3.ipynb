{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39998176-0acb-4c33-8993-11ce5c775be8",
   "metadata": {},
   "source": [
    "## Lesson 3: Building a Simple Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e3964b",
   "metadata": {},
   "source": [
    "Before you transform your RAG prototype into an automated pipeline, you will learn some basic Airflow syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2846db35",
   "metadata": {},
   "source": [
    "### 3.1. Airflow UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7021cab",
   "metadata": {},
   "source": [
    "You will use the Airflow UI to visualize the dags, track their status and trigger them manually. Run the cell below to get the link to your Airflow UI. If asked for username and password, type `airflow` for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13b897c-31ce-4080-95e4-37c553a65d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "airflow_ui = os.environ.get('DLAI_LOCAL_URL').format(port=8080)\n",
    "airflow_ui #username:airflow password:airflow (if asked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4755789",
   "metadata": {},
   "source": [
    "### 3.2. Airflow Components - Optional Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407ce221",
   "metadata": {},
   "source": [
    "You've already seen one of the Airflow components which is the Airflow UI hosted on the API server which is shown in the diagram below. Airflow has other components that interact all together to process and run the dags you write. In this course, the components are already set up for you (each component is running in a docker container). If you'd like to know how to install Airflow locally on your machine, please check the resource section below in this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb1fbed",
   "metadata": {},
   "source": [
    "<img src=\"airflow_architecture_3.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949fc043",
   "metadata": {},
   "source": [
    "You will write your dags as python files and save them in a dags folder. Once you add a new dag to your Airflow environment:\n",
    "1. The dag processor parses your dag and stores a serialized version of the dag in the Airflow metadata database.\n",
    "2. The scheduler checks the serialized dags to determine whether any dag is eligible for execution based on its defined schedule.\n",
    "3. The tasks are then scheduled and subsequently queued. The workers poll the queue for any queued task instances they can run.\n",
    "4. The worker who picked up the task instance runs it, and metadata such as the task instance status is sent from the worker via the API server to be stored in the Airflow metadata database. \n",
    "5. Some of this information, such as the task instance status, is in turn important for the scheduler. It monitors all dags and, as soon as their dependencies are fulfilled, schedules task instances to run.\n",
    "\n",
    "While this process is going on in the background, the Airflow UI, served by the API server, displays information about the current dag and task statuses that it retrieves from the Airflow metadata database.\n",
    "\n",
    "If you'd like to learn about Airflow components, you can check chapter 5 of [this practical guide](https://www.astronomer.io/ebooks/practical-guide-to-apache-airflow-3/?utm_source=deeplearning-ai&utm_medium=content&utm_campaign=genai-course-6-25)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a183b1af",
   "metadata": {},
   "source": [
    "### 3.3. Your First DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fc60a6",
   "metadata": {},
   "source": [
    "You'll now write your first day. The magic command `%%writefile` copies the content of the cell to the file `my_first_dag.py` stored under the `dags` folder. The `dags` folder, which is provided to you in this lab environment, will be automatically checked by the dag processor. Once the dag processor finds `my_first_dag.py`, it will automatically parse it and you can then view it in the Airflow UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb719ac-a8ed-49eb-a0fc-22bcc0e1eb8c",
   "metadata": {},
   "source": [
    "#### 3.3.1. My first dag with 2 tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb94d5b8",
   "metadata": {},
   "source": [
    "Run the following cell. After around 30 seconds, you should see the first dag in the UI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d23d64-f798-46f2-94c4-ffc5791bd460",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../../dags/my_first_dag.py\n",
    "\n",
    "from airflow.sdk import dag, task, chain\n",
    "\n",
    "\n",
    "@dag\n",
    "def my_first_dag():\n",
    "\n",
    "    @task\n",
    "    def my_task_1():\n",
    "        return {\"my_word\" : \"Airflow!\"}\n",
    "    \n",
    "    _my_task_1 = my_task_1()\n",
    "\n",
    "    @task \n",
    "    def my_task_2(my_dict):\n",
    "        print(my_dict[\"my_word\"])\n",
    "\n",
    "    _my_task_2 = my_task_2(my_dict=_my_task_1)\n",
    "\n",
    "    \n",
    "my_first_dag()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a55ca24",
   "metadata": {},
   "source": [
    "**Note**: Where is the `dags` folder? In this environment, the `dags` folder lives at this address: `/home/jovyan/dags` (not in the lesson folders). You don't have direct access to the `dags` folder; but if you want to download all the dag files of this course, you can find them in this [github repo](https://github.com/astronomer/orchestrating-workflows-for-genai-deeplearning-ai). The repo also contains instructions on how to run Airflow locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2098d5f0-1daf-4908-967c-db3ddab7c388",
   "metadata": {},
   "source": [
    "#### 3.3.2. My first dag with 3 tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c6ece0",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ‚è≥ <b>Note <code>Airflow UI</code>:</b> \n",
    "\n",
    "<p>Changes to dags may take up to 30 seconds to show up in the Airflow UI in this environment! </p>\n",
    "<p>In the Airflow UI, if you see the error \"504 Gateway Timeout\", this can happen after 2 hours or after some time of inactivity 25 minutes (if there's no activity for 20 minutes, the jupyter kernel stops and if there's no kernel for 5 minutes, then the jupyter notebook stops and the resources are released). In this case, make sure to refresh the notebook, run the cell that outputs the link to the Airflow UI and then use the link to open the Airflow UI. </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742b243d-21e6-411d-8330-ba12b363e01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../../dags/my_first_dag.py\n",
    "\n",
    "from airflow.sdk import dag, task, chain\n",
    "\n",
    "\n",
    "@dag\n",
    "def my_first_dag():\n",
    "\n",
    "    @task\n",
    "    def my_task_1():\n",
    "        return {\"my_word\" : \"Airflow!\"}\n",
    "    \n",
    "    _my_task_1 = my_task_1()\n",
    "\n",
    "    @task \n",
    "    def my_task_2(my_dict):\n",
    "        print(my_dict[\"my_word\"])\n",
    "\n",
    "    _my_task_2 = my_task_2(my_dict=_my_task_1)\n",
    "\n",
    "    @task \n",
    "    def my_task_3():\n",
    "        print(\"Hi from my_task_3!\")\n",
    "\n",
    "    _my_task_3 = my_task_3()\n",
    "\n",
    "    chain(_my_task_1, _my_task_3)   \n",
    "        \n",
    "\n",
    "my_first_dag()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c300244f",
   "metadata": {},
   "source": [
    "### 3.4. Your Second DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79788598-a44b-478b-8e48-5df1011645ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../../dags/my_second_dag.py  \n",
    "from airflow.sdk import dag, task, chain\n",
    "\n",
    "\n",
    "@dag\n",
    "def my_second_dag():\n",
    "    @task\n",
    "    def my_task_1():\n",
    "        return 23\n",
    "\n",
    "    _my_task_1 = my_task_1()\n",
    "\n",
    "    @task\n",
    "    def my_task_2():\n",
    "        return 42\n",
    "\n",
    "    _my_task_2 = my_task_2()\n",
    "\n",
    "    @task\n",
    "    def my_task_3(num1, num2):\n",
    "        return num1 + num2\n",
    "\n",
    "    _my_task_3 = my_task_3(num1=_my_task_1, num2=_my_task_2)\n",
    "\n",
    "    @task\n",
    "    def my_task_4():\n",
    "        return \"Math!\"\n",
    "\n",
    "    _my_task_4 = my_task_4()\n",
    "\n",
    "    chain([_my_task_2, _my_task_3], _my_task_4)\n",
    "\n",
    "\n",
    "my_second_dag()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6006524c",
   "metadata": {},
   "source": [
    "### 3.5. Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c077f6a2",
   "metadata": {},
   "source": [
    "How to install Airflow locally:\n",
    "\n",
    "- If you're familiar with running Docker containers, you can check this guide: [Running Airflow in Docker](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html)\n",
    "- If you'd like an easier approach to start with Airflow, you can use [Astro CLI](https://www.astronomer.io/docs/astro/cli/get-started-cli):\n",
    "  - Make sure to check the last optional video of this course \"How to Set up a Local Airflow Environment\" that shows you how to replicate the same lab environment locally. It has this companion [github repo](https://github.com/astronomer/orchestrating-workflows-for-genai-deeplearning-ai).\n",
    "\n",
    "Airflow features:  \n",
    "\n",
    "- [Introduction to the TaskFlow API and Airflow decorators](https://www.astronomer.io/docs/learn/airflow-decorators/): Learn more about decorators generally in Python and specifically in Airflow.\n",
    "- [Manage task and task group dependencies in Airflow](https://www.astronomer.io/docs/learn/managing-dependencies/): Learn more about setting dependencies between tasks using the `chain` function and other methods.\n",
    "- [Airflow Operators](https://www.astronomer.io/docs/learn/what-is-an-operator): Learn more about operator classes which can be used alongside `@task` to create Airflow tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576d892c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "\n",
    "<p> ‚¨á &nbsp; <b>Download Notebooks:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Download as\"</em> and select <em>\"Notebook (.ipynb)\"</em>.</p>\n",
    "\n",
    "<p> üìí &nbsp; For more help, please see the <em>\"Appendix ‚Äì Tips, Help, and Download\"</em> Lesson.</p>\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
