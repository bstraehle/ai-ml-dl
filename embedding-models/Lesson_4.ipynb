{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "497db21d-4e08-42a7-9258-90fecced929f",
   "metadata": {},
   "source": [
    "# L4: Training a Dual Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4cf2e4-def8-44c7-a5f2-cf88836e9e22",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ‚è≥ <b>Note <code>(Kernel Starting)</code>:</b> This notebook takes about 30 seconds to be ready to use. You may start and watch the video while you wait.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5725255c-8e4e-459b-ab12-29fca393b662",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23226224-1ca8-4ab3-b8b6-ffd560b205e4",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6fc6d7-218b-44a3-ac58-6ef41b5faf8f",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6ff; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\"> üíª &nbsp; <b>Access <code>requirements.txt</code> file:</b> To access <code>requirements.txt</code> for this notebook, 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em>. For more help, please see the <em>\"Appendix - Tips and Help\"</em> Lesson.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ce87a1-5ca5-41ec-b7d2-e44a892f7b9a",
   "metadata": {},
   "source": [
    "## The CrossEntropyLoss 'trick'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b7afc1-6894-42e0-a3c9-479372f1da38",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    [\n",
    "        [4.3, 1.2, 0.05, 1.07],\n",
    "        [0.18, 3.2, 0.09, 0.05],\n",
    "        [0.85, 0.27, 2.2, 1.03],\n",
    "        [0.23, 0.57, 0.12, 5.1]\n",
    "    ]\n",
    ")\n",
    "data = torch.tensor(df.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916a47b2-ece3-4719-a8bb-162f3a1fd33d",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "def contrastive_loss(data):\n",
    "    target = torch.arange(data.size(0))\n",
    "    loss = torch.nn.CrossEntropyLoss()(data, target)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b4d461-4905-4a3b-9f49-5373cfa64b7e",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "torch.nn.Softmax(dim=1)(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf89b540-550a-47cb-a4a5-502d6544d949",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "torch.nn.Softmax(dim=1)(data).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bc91c4-fe9f-44d7-8735-c554ec3c4bfd",
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": [
    "N = data.size(0)\n",
    "non_diag_mask = ~torch.eye(N, N, dtype=bool)\n",
    "\n",
    "for inx in range(3):\n",
    "    data = torch.tensor(df.values, dtype=torch.float32)\n",
    "    data[range(N), range(N)] += inx*0.5\n",
    "    data[non_diag_mask] -= inx*0.02\n",
    "    print(data)\n",
    "    print(f\"Loss = {contrastive_loss(data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5d4e24-3cb2-416b-8705-9d28e86a6b86",
   "metadata": {},
   "source": [
    "## The Encoder module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3254ee3-4aef-4dca-802f-334275a06d05",
   "metadata": {
    "height": 302
   },
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, output_embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.encoder = torch.nn.TransformerEncoder(\n",
    "            torch.nn.TransformerEncoderLayer(embed_dim, nhead=8, batch_first=True),\n",
    "            num_layers=3,\n",
    "            norm=torch.nn.LayerNorm([embed_dim]),\n",
    "            enable_nested_tensor=False\n",
    "        )\n",
    "        self.projection = torch.nn.Linear(embed_dim, output_embed_dim)\n",
    "    \n",
    "    def forward(self, tokenizer_output):\n",
    "        x = self.embedding_layer(tokenizer_output['input_ids'])\n",
    "        x = self.encoder(x, src_key_padding_mask=tokenizer_output['attention_mask'].logical_not())\n",
    "        cls_embed = x[:,0,:]\n",
    "        return self.projection(cls_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435d2b75-1e6b-4a25-ba17-adcca6f1b11f",
   "metadata": {},
   "source": [
    "![Diagram 1](DLAI-diagram-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7195b4-d7d2-4bf6-850f-a7793afca09b",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fdb948-7ee0-46c6-8423-1bb6c1160bf7",
   "metadata": {
    "height": 812
   },
   "outputs": [],
   "source": [
    "def train_loop(dataset):\n",
    "    embed_size = 512\n",
    "    output_embed_size = 128\n",
    "    max_seq_len = 64\n",
    "    batch_size = 32\n",
    "\n",
    "    # define the question/answer encoders\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    question_encoder = Encoder(tokenizer.vocab_size, embed_size, \n",
    "                               output_embed_size)\n",
    "    answer_encoder = Encoder(tokenizer.vocab_size, embed_size, \n",
    "                             output_embed_size)\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                             shuffle=True)    \n",
    "    optimizer = torch.optim.Adam(\n",
    "        list(question_encoder.parameters()) + list(answer_encoder.parameters()\n",
    "    ), lr=1e-5)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    running_loss = []\n",
    "    for _, data_batch in enumerate(dataloader):\n",
    "\n",
    "        # Tokenize the question/answer pairs (each is a batch of 32 questions and 32 answers)\n",
    "        question, answer = data_batch\n",
    "        question_tok = tokenizer(question, padding=True, truncation=True, return_tensors='pt', max_length=max_seq_len)\n",
    "        answer_tok = tokenizer(answer, padding=True, truncation=True, return_tensors='pt', max_length=max_seq_len)\n",
    "\n",
    "        # Compute the embeddings: the output is of dim = 32 x 128\n",
    "        question_embed = question_encoder(question_tok)\n",
    "        answer_embed = answer_encoder(answer_tok)\n",
    "\n",
    "        # Compute similarity scores: a 32x32 matrix\n",
    "        # row[N] reflects similarity between question[N] and answers[0...31]\n",
    "        similarity_scores = question_embed @ answer_embed.T\n",
    "\n",
    "        # we want to maximize the values in the diagonal\n",
    "        target = torch.arange(question_embed.shape[0], dtype=torch.long)\n",
    "        loss = loss_fn(similarity_scores, target)\n",
    "        running_loss += [loss.item()]\n",
    "\n",
    "        # this is where the magic happens\n",
    "        optimizer.zero_grad()    # reset optimizer so gradients are all-zero\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return question_encoder, answer_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61f33a1-6615-460c-af10-17aaa63b7797",
   "metadata": {},
   "source": [
    "## Training in multiple Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff76b1a9-02f3-4090-a1cb-e494cbe7ea8f",
   "metadata": {
    "height": 931
   },
   "outputs": [],
   "source": [
    "def train(dataset, num_epochs=10):\n",
    "    embed_size = 512\n",
    "    output_embed_size = 128\n",
    "    max_seq_len = 64\n",
    "    batch_size = 32\n",
    "\n",
    "    n_iters = len(dataset) // batch_size + 1\n",
    "    \n",
    "    # define the question/answer encoders\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    question_encoder = Encoder(tokenizer.vocab_size, embed_size, output_embed_size)\n",
    "    answer_encoder = Encoder(tokenizer.vocab_size, embed_size, output_embed_size)\n",
    "\n",
    "    # define the dataloader, optimizer and loss function    \n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)    \n",
    "    optimizer = torch.optim.Adam(list(question_encoder.parameters()) + list(answer_encoder.parameters()), lr=1e-5)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = []\n",
    "        for idx, data_batch in enumerate(dataloader):\n",
    "\n",
    "            # Tokenize the question/answer pairs (each is a batc of 32 questions and 32 answers)\n",
    "            question, answer = data_batch\n",
    "            question_tok = tokenizer(question, padding=True, truncation=True, return_tensors='pt', max_length=max_seq_len)\n",
    "            answer_tok = tokenizer(answer, padding=True, truncation=True, return_tensors='pt', max_length=max_seq_len)\n",
    "            if inx == 0 and epoch == 0:\n",
    "                print(question_tok['input_ids'].shape, answer_tok['input_ids'].shape)\n",
    "            \n",
    "            # Compute the embeddings: the output is of dim = 32 x 128\n",
    "            question_embed = question_encoder(question_tok)\n",
    "            answer_embed = answer_encoder(answer_tok)\n",
    "            if inx == 0 and epoch == 0:\n",
    "                print(question_embed.shape, answer_embed.shape)\n",
    "    \n",
    "            # Compute similarity scores: a 32x32 matrix\n",
    "            # row[N] reflects similarity between question[N] and answers[0...31]\n",
    "            similarity_scores = question_embed @ answer_embed.T\n",
    "            if inx == 0 and epoch == 0:\n",
    "                print(similarity_scores.shape)\n",
    "    \n",
    "            # we want to maximize the values in the diagonal\n",
    "            target = torch.arange(question_embed.shape[0], dtype=torch.long)\n",
    "            loss = loss_fn(similarity_scores, target)\n",
    "            running_loss += [loss.item()]\n",
    "            if idx == n_iters-1:\n",
    "                print(f\"Epoch {epoch}, loss = \", np.mean(running_loss))\n",
    "    \n",
    "            # this is where the magic happens\n",
    "            optimizer.zero_grad()    # reset optimizer so gradients are all-zero\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return question_encoder, answer_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918ce22f-b921-49af-b932-16e9926f945c",
   "metadata": {},
   "source": [
    "## Let's train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9733e9a-2704-4bb1-b7ff-0d6c710d688a",
   "metadata": {
    "height": 217
   },
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, datapath):\n",
    "        self.data = pd.read_csv(datapath, sep=\"\\t\", nrows=300)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data.iloc[idx]['questions'], self.data.iloc[idx]['answers']\n",
    "\n",
    "dataset = MyDataset('./shared_data/nq_sample.tsv')\n",
    "dataset.data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb3d80-8635-4800-83c2-ca64991b98dd",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ‚è≥ <b>Note <code>(num_epochs = 5)</code>:</b> The <code>num_epochs</code> is set to <code>5</code> for speedier execution. You may train the model \n",
    "using a higher number of epochs by changing this parameter.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eca1874-c660-4336-b411-5ab470b0a9b5",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "qe, ae = train(dataset, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c17cfc-02ad-4a02-99fc-da55da8c17ec",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "question = 'What is the tallest mountain in the world?'\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "question_tok = tokenizer(question, padding=True, truncation=True, return_tensors='pt', max_length=64)\n",
    "question_emb = qe(question_tok)[0]\n",
    "print(question_tok)\n",
    "print(question_emb[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a33357-6507-4ae9-a638-2de4af933c86",
   "metadata": {
    "height": 302
   },
   "outputs": [],
   "source": [
    "answers = [\n",
    "    \"What is the tallest mountain in the world?\",\n",
    "    \"The tallest mountain in the world is Mount Everest.\",\n",
    "    \"Who is donald duck?\"\n",
    "]\n",
    "answer_tok = []\n",
    "answer_emb = []\n",
    "for answer in answers:\n",
    "    tok = tokenizer(answer, padding=True, truncation=True, return_tensors='pt', max_length=64)\n",
    "    answer_tok.append(tok['input_ids'])\n",
    "    emb = ae(tok)[0]\n",
    "    answer_emb.append(emb)\n",
    "\n",
    "print(answer_tok)\n",
    "print(answer_emb[0][:5])\n",
    "print(answer_emb[1][:5])\n",
    "print(answer_emb[2][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5a9a16-016c-4240-9a3f-847c5a7d5624",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "question_emb @ torch.stack(answer_emb).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789b0aa3-7a3b-4618-bc7e-14d18492a42d",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7fd33c-c5f2-466a-b011-b6dc8495c2be",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d282a2-0a68-4ef1-baab-15a1ec809741",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd2243d-0984-42ac-92a1-81a20dd4df2d",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbdc3b8-b5cc-40ec-83a0-d4c56fd481b4",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
