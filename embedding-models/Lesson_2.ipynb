{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "497db21d-4e08-42a7-9258-90fecced929f",
   "metadata": {},
   "source": [
    "# L2 - Contextualized Token Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d993bfb-80c1-4817-8622-a065d71da414",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ‚è≥ <b>Note <code>(Kernel Starting)</code>:</b> This notebook takes about 30 seconds to be ready to use. You may start and watch the video while you wait.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372ca766-d5c2-409e-9385-1c1b2b775f86",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23226224-1ca8-4ab3-b8b6-ffd560b205e4",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333e993d-9d96-4b33-9f16-5cd4e1257d94",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6ff; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\"> üíª &nbsp; <b>Access <code>requirements.txt</code> file:</b> To access <code>requirements.txt</code> for this notebook, 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em>. For more help, please see the <em>\"Appendix - Tips and Help\"</em> Lesson.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc50c2b6-0745-4952-83c8-170275da0e2d",
   "metadata": {},
   "source": [
    "## GloVe word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b7afc1-6894-42e0-a3c9-479372f1da38",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "word_vectors = api.load('glove-wiki-gigaword-100')\n",
    "#word_vectors = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37949be5-9ee4-4b29-ba46-df28a0c5d792",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "word_vectors['king'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916a47b2-ece3-4719-a8bb-162f3a1fd33d",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "word_vectors['king'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b4d461-4905-4a3b-9f49-5373cfa64b7e",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "# Words to visualize\n",
    "words = [\"king\", \"princess\", \"monarch\", \"throne\", \"crown\", \n",
    "         \"mountain\", \"ocean\", \"tv\", \"rainbow\", \"cloud\", \"queen\"]\n",
    "\n",
    "# Get word vectors\n",
    "vectors = np.array([word_vectors[word] for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf89b540-550a-47cb-a4a5-502d6544d949",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# Reduce dimensions using PCA\n",
    "pca = PCA(n_components=2)\n",
    "vectors_pca = pca.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359e1719-dd9b-4130-93f1-8cd2cf97e2d6",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig, axes = plt.subplots(1, 1, figsize=(5, 5))\n",
    "axes.scatter(vectors_pca[:, 0], vectors_pca[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    axes.annotate(word, (vectors_pca[i, 0]+.02, vectors_pca[i, 1]+.02))\n",
    "axes.set_title('PCA of Word Embeddings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6541fd-4b2c-40f3-b52b-feb052f8154b",
   "metadata": {},
   "source": [
    "## Word2Vec algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fdb948-7ee0-46c6-8423-1bb6c1160bf7",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "result = word_vectors.most_similar(positive=['king', 'woman'], \n",
    "                                   negative=['man'], topn=1)\n",
    "\n",
    "# Output the result\n",
    "print(f\"\"\"\n",
    "    The word closest to 'king' - 'man' + 'woman' is: '{result[0][0]}' \n",
    "    with a similarity score of {result[0][1]}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca418d70-69ae-479c-b174-f61a69a834ba",
   "metadata": {},
   "source": [
    "## GloVe vs BERT: words in context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02a644c-9413-407a-aa6b-d5a5079dbb76",
   "metadata": {
    "height": 217
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./models/bert-base-uncased')\n",
    "model = BertModel.from_pretrained('./models/bert-base-uncased')\n",
    "\n",
    "# Function to get BERT embeddings\n",
    "def get_bert_embeddings(sentence, word):\n",
    "    inputs = tokenizer(sentence, return_tensors='pt')\n",
    "    outputs = model(**inputs)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    word_tokens = tokenizer.tokenize(sentence)\n",
    "    word_index = word_tokens.index(word)\n",
    "    word_embedding = last_hidden_states[0, word_index + 1, :]  # +1 to account for [CLS] token\n",
    "    return word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449f1563-c1ec-497d-a736-aec65dc5efb1",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "sentence1 = \"The bat flew out of the cave at night.\"\n",
    "sentence2 = \"He swung the bat and hit a home run.\"\n",
    "\n",
    "word = \"bat\"\n",
    "\n",
    "bert_embedding1 = get_bert_embeddings(sentence1, word).detach().numpy()\n",
    "bert_embedding2 = get_bert_embeddings(sentence2, word).detach().numpy()\n",
    "word_embedding = word_vectors[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e182e2-9c92-40e6-bd57-5650d94789f2",
   "metadata": {
    "height": 200
   },
   "outputs": [],
   "source": [
    "print(\"BERT Embedding for 'bat' in sentence 1:\", bert_embedding1[:5])\n",
    "print(\"BERT Embedding for 'bat' in sentence 2:\", bert_embedding2[:5])\n",
    "print(\"GloVe Embedding for 'bat':\", word_embedding[:5])\n",
    "\n",
    "bert_similarity = cosine_similarity([bert_embedding1], [bert_embedding2])[0][0]\n",
    "word_embedding_similarity = cosine_similarity([word_embedding], [word_embedding])[0][0]\n",
    "\n",
    "print()\n",
    "print(f\"Cosine Similarity between BERT embeddings in different contexts: {bert_similarity}\")\n",
    "print(f\"Cosine Similarity between GloVe embeddings: {word_embedding_similarity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a2afb8-f892-4a91-bbc7-c0f3b280fa53",
   "metadata": {},
   "source": [
    "## Cross Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789b0aa3-7a3b-4618-bc7e-14d18492a42d",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', max_length=512, \n",
    "                     default_activation_function=torch.nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7fd33c-c5f2-466a-b011-b6dc8495c2be",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "question = \"Where is the capital of France?\"\n",
    "# Define sentences to compare\n",
    "answers = [\n",
    "    \"Paris is the capital of France.\",\n",
    "    \"Berlin is the capital of Germany.\",\n",
    "    \"Madrid is the capital of Spain.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5a7ebb-1014-45a2-8e28-2026af971189",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "scores = model.predict([(question, answers[0]), (question, answers[1]),\n",
    "                        (question, answers[2])])\n",
    "print(scores)\n",
    "most_relevant_idx = torch.argmax(torch.tensor(scores)).item()\n",
    "print(f\"The most relevant passage is: {answers[most_relevant_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf83a7f-daaf-4a68-83c4-49efdb58c898",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f774b953-b8dc-434f-992d-95439cb40a5a",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e3d4ad-f88f-4992-a004-57ecff651389",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4117b1d7-4a62-4ea3-a37b-39410965922d",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74375022-53c8-4952-ad17-74e0774764c8",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
