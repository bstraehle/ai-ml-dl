{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3416ed55-4e38-4cdd-afe0-e14aa7d857b4",
   "metadata": {},
   "source": [
    "# L3: Create an Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec741e76-1cc8-46b2-8cc8-dd65072bc78f",
   "metadata": {},
   "source": [
    "> Note: You can access the `data` and `util` subdirectories used in the course. In Jupyter version 6, this is via the File>Open menu. In Jupyter version 7 this is in View> File Browser\n",
    "\n",
    "> Also note that as models and systems change, the output of the models may shift from the video content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5745ff93-d7a6-4bc6-82eb-ca0da00f1d05",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()   #load environmental variable LAMINI_API_KEY with key from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759c10e6-f528-4431-8676-0317312427c2",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "!cat data/gold-test-set.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83fcc4c-b1c1-4121-97e9-6c1dd5414b16",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "question = \"What is the median weight in the NBA?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6af2fb-00cb-4ee1-8c96-e7bcb42574b3",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "import lamini "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7879b875-3ae5-4f46-b4d1-6dedaad2385a",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from util.get_schema import get_schema\n",
    "from util.make_llama_3_prompt import make_llama_3_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3501c5a-094d-4fd2-9b15-ce7d108b0d43",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "llm = lamini.Lamini(model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b5065e-d4eb-43a7-bd8f-efbc294a7dcc",
   "metadata": {
    "height": 113
   },
   "outputs": [],
   "source": [
    "system = f\"\"\"You are an NBA analyst with 15 years of experience writing complex SQL queries. Consider the nba_roster table with the following schema:\n",
    "{get_schema()}\n",
    "\n",
    "Write a sqlite query to answer the following question. Follow instructions exactly\"\"\"\n",
    "prompt = make_llama_3_prompt(question, system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7331bf-fe90-46aa-af6b-948241ba4369",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "generated_query = llm.generate(prompt, output_type={\"sqlite_query\": \"str\"}, max_new_tokens=200)\n",
    "print(generated_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980367c9-993d-41af-8667-0bf1da622929",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "engine = sqlite3.connect(\"./nba_roster.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa478584-2edf-44c9-9d6f-b597be6cb022",
   "metadata": {},
   "source": [
    "> The following cell is expected to create an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63d1e4c-21ec-4c8a-9074-7860695ef15d",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "df = pd.read_sql(generated_query['sqlite_query'], con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560a20a5-bf9a-4e55-8bc3-424899981032",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "engine = sqlite3.connect(\"./nba_roster.db\")\n",
    "try:\n",
    "    df = pd.read_sql(generated_query['sqlite_query'], con=engine)\n",
    "    print(df)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552da86b-71db-4caf-b18a-d8c07177b908",
   "metadata": {},
   "source": [
    "### Try Agent Reflection to see if that can improve the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fd8cbe-84ab-4888-a77c-b0a2ab1f24c2",
   "metadata": {
    "height": 45
   },
   "outputs": [],
   "source": [
    "reflection = f\"Question: {question}. Query: {generated_query['sqlite_query']}. This query is invalid (gets the error Execution failed on sql 'SELECT AVG(CAST(SUBSTR(WT, INSTR(WT,'') + 1) AS INTEGER) FROM nba_roster WHERE WT IS NOT NULL': near \\\"FROM\\\": syntax error), so it cannot answer the question. Write a corrected sqlite query.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a5f816-b17d-4cfb-b706-ea008c26532f",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "reflection_prompt = make_llama_3_prompt(reflection, system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ecba8f-4919-4b13-b053-02e7c35fcf24",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "reflection_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49952fa4-8690-4780-8b10-b0fc97710733",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "reflection_query = llm.generate(reflection_prompt, output_type={\"sqlite_query\": \"str\"}, max_new_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c55d269-46bd-4650-ad08-5050cfdaa954",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "reflection_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407f1212-b95d-46a1-8673-ea8fb8260a75",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_sql(reflection_query['sqlite_query'], con=engine)\n",
    "    print(df)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3926bfb-6f48-49ee-a48a-09fd70e246a0",
   "metadata": {},
   "source": [
    "### Look at the right answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80f44f9-86fe-41c4-9955-426da15b2aef",
   "metadata": {
    "height": 45
   },
   "outputs": [],
   "source": [
    "correct_sql = \"select CAST(SUBSTR(WT, 1, INSTR(WT,' ')) as INTEGER) as percentile from nba_roster order by percentile limit 1 offset (select count(*) from nba_roster)/2;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f111edf8-a69f-45e0-b67a-6891827f396d",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "correct_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa638ca-ae31-477c-9583-ba59fedcb829",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "df_corrected = pd.read_sql(correct_sql, con=engine)\n",
    "print(df_corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c7a7e0-0de1-43dc-a866-299dda7ef6e6",
   "metadata": {},
   "source": [
    "### Evaluate over a larger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6c36c4-60ba-4068-93f4-cc5333d88e35",
   "metadata": {
    "height": 608
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from typing import AsyncIterator, Iterator, Union\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "from lamini.generation.base_prompt_object import PromptObject\n",
    "from lamini.generation.generation_node import GenerationNode\n",
    "from lamini.generation.base_prompt_object import PromptObject\n",
    "from lamini.generation.generation_pipeline import GenerationPipeline\n",
    "from util.get_schema import get_schema\n",
    "from util.make_llama_3_prompt import make_llama_3_prompt\n",
    "from util.setup_logging import setup_logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "engine = sqlite3.connect(\"./nba_roster.db\")\n",
    "setup_logging()\n",
    "\n",
    "class Args:\n",
    "    def __init__(self, \n",
    "                 max_examples=100, \n",
    "                 sql_model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\", \n",
    "                 gold_file_name=\"gold-test-set.jsonl\",\n",
    "                 training_file_name=\"archive/generated_queries.jsonl\",\n",
    "                 num_to_generate=10):\n",
    "        self.sql_model_name = sql_model_name\n",
    "        self.max_examples = max_examples\n",
    "        self.gold_file_name = gold_file_name\n",
    "        self.training_file_name = training_file_name\n",
    "        self.num_to_generate = num_to_generate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6314b56a-b4b1-4247-806e-9c6332614a02",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "def load_gold_dataset(args):\n",
    "    path = f\"data/{args.gold_file_name}\"\n",
    "\n",
    "    with jsonlines.open(path) as reader:\n",
    "        for index, obj in enumerate(reversed(list(reader))):\n",
    "            if index >= args.max_examples:\n",
    "                break\n",
    "            yield PromptObject(prompt=\"\", data=obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0529b4d-e49d-41e7-9f47-a358bddd1b93",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "path = \"data/gold-test-set.jsonl\"\n",
    "\n",
    "with jsonlines.open(path) as reader:\n",
    "    data = [obj for obj in reader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5536b611-a63f-4a49-8303-3c70e6de18db",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "datapoint = data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a585d974-0601-4ab3-a289-66059c118d82",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f87426-b2af-475f-bbab-ef2407768d81",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "datapoint = data[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63e2e8a-031a-4a70-9db8-479c1409e08d",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324b608a-75ac-4404-ac55-41712efe9d1f",
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": [
    "system = \"You are an NBA analyst with 15 years of experience writing complex SQL queries.\\n\"\n",
    "system += \"Consider the nba_roster table with the following schema:\\n\"\n",
    "system += get_schema() + \"\\n\"\n",
    "system += (\n",
    "    \"Write a sqlite SQL query that would help you answer the following question:\\n\"\n",
    ")\n",
    "user = datapoint[\"question\"]\n",
    "prompt = make_llama_3_prompt(user, system)\n",
    "generated_sql = llm.generate(prompt, output_type={\"sqlite_query\": \"str\"}, max_new_tokens=200)\n",
    "print(generated_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c51b433-9137-4230-9a7b-bab3c24fa23f",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "df = pd.read_sql(generated_sql['sqlite_query'], con=engine)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e051553d-7c78-4580-a9d4-2ac4dafd1c12",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "query_succeeded = False\n",
    "try:\n",
    "    df = pd.read_sql(generated_sql['sqlite_query'], con=engine)\n",
    "    query_succeeded = True\n",
    "    print(\"Query is valid\")\n",
    "except Exception as e:\n",
    "    print(\n",
    "        f\"Failed to run SQL query: {generated_sql}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9a3cc0-6ff3-4037-8913-eb373e0e1249",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "reference_sql = datapoint[\"sql\"]\n",
    "ref_df = pd.read_sql(reference_sql, con=engine)\n",
    "print(ref_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e2d58b-aaec-4fc4-8435-148fc4b28093",
   "metadata": {
    "height": 1186
   },
   "outputs": [],
   "source": [
    "# Here's how to wrap that all up in a runnable class\n",
    "\n",
    "class QueryStage(GenerationNode):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__(\n",
    "            model_name=model_name,\n",
    "            max_new_tokens=200,\n",
    "        )\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: Union[Iterator[PromptObject], AsyncIterator[PromptObject]],\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        results = super().generate(\n",
    "            prompt,\n",
    "            output_type={\"sqlite_query\": \"str\"},\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "        return results\n",
    "\n",
    "\n",
    "    def postprocess(self, obj: PromptObject):\n",
    "        # Run both the generated and reference (Gold Dataset) SQL queries\n",
    "        # Assessing whether the SQL queries succeeded in hitting the database (not correctness yet!)\n",
    "        \n",
    "        query_succeeded = False\n",
    "\n",
    "        try:\n",
    "            logger.error(f\"Running SQL query '{obj.response['sqlite_query']}'\")\n",
    "            obj.data[\"generated_query\"] = obj.response[\"sqlite_query\"]\n",
    "            df = pd.read_sql(obj.response[\"sqlite_query\"], con=engine)\n",
    "            obj.data['df'] = df\n",
    "            logger.error(f\"Got data: {df}\")\n",
    "            query_succeeded = True\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\n",
    "                f\"Failed to run SQL query: {obj.response['sqlite_query']}\"\n",
    "            )\n",
    "\n",
    "        logger.info(f\"Running reference SQL query '{obj.data['sql']}'\")\n",
    "        df = pd.read_sql(obj.data[\"sql\"], con=engine)\n",
    "        logger.info(f\"Got data: {df}\")\n",
    "        obj.data['reference_df'] = df\n",
    "\n",
    "        logger.info(f\"For question: {obj.data['question']}\")\n",
    "        logger.info(f\"For query: {obj.response['sqlite_query']}\")\n",
    "\n",
    "        obj.data[\"query_succeeded\"] = query_succeeded\n",
    "\n",
    "    def preprocess(self, obj: PromptObject):\n",
    "        new_prompt = make_llama_3_prompt(**self.make_prompt(obj.data))\n",
    "        obj.prompt = new_prompt\n",
    "\n",
    "    def make_prompt(self, data: dict):\n",
    "        system = \"You are an NBA analyst with 15 years of experience writing complex SQL queries.\\n\"\n",
    "        system += \"Consider the nba_roster table with the following schema:\\n\"\n",
    "        system += get_schema() + \"\\n\"\n",
    "        system += (\n",
    "            \"Write a sqlite SQL query that would help you answer the following question:\\n\"\n",
    "        )\n",
    "        user = data[\"question\"]\n",
    "        return {\n",
    "            \"user\": user,\n",
    "            \"system\": system,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be880417-09d3-497b-9596-451b798afeab",
   "metadata": {},
   "source": [
    "Compare strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cba156-4b6b-499b-8917-28e3b0d9117f",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "str(df).lower() == str(ref_df).lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65781433-acfc-46f3-8c98-1295fb84d6ca",
   "metadata": {},
   "source": [
    "Use an LLM to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08205a44-2b1d-417a-9748-d5b5754ac3b9",
   "metadata": {
    "height": 79
   },
   "outputs": [],
   "source": [
    "system_prompt = \"Compare the following two dataframes. They are similar if they are almost identical, or if they convey the same information about the nba_roster dataset\"\n",
    "system_prompt += \"Respond with valid JSON {'explanation' : str, 'similar' : bool}\"\n",
    "system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365bca3a-f63c-40f9-b872-7811d8cb6323",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "user_prompt = (\n",
    "    f\"========== Dataframe 1 =========\\n{str(df).lower()}\\n\\n\"\n",
    ")\n",
    "user_prompt += (\n",
    "    f\"========== Dataframe 2 =========\\n{str(ref_df).lower()}\\n\\n\"\n",
    ")\n",
    "user_prompt += f\"Can you tell me if these dataframes are similar?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c75f6d-98ce-4b2d-a888-2b8a7f24ce10",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "llm_similarity_prompt = make_llama_3_prompt(user_prompt, system_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28440158-0f30-4372-9ee4-ebea8786382e",
   "metadata": {
    "height": 62
   },
   "outputs": [],
   "source": [
    "llm_similarity = llm.generate(llm_similarity_prompt, output_type={\"explanation\": \"str\", \"similar\": \"bool\"}, max_new_tokens=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9be7072-ddc5-4867-a507-09410d955d1e",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "llm_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b28fdd-24fb-4cd5-bb02-7bed9683596b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "str(df).lower() == str(ref_df).lower() or llm_similarity[\"similar\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff72ef3-848c-4f9d-8b4c-fac716124d78",
   "metadata": {
    "height": 980
   },
   "outputs": [],
   "source": [
    "# How to wrap it up in a class\n",
    "\n",
    "class ScoreStage(GenerationNode):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "            max_new_tokens=150,\n",
    "        )\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: Union[Iterator[PromptObject], AsyncIterator[PromptObject]],\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        logger.debug(\"ScoreStage Generate\")\n",
    "        results = super().generate(\n",
    "            prompt,\n",
    "            output_type={\"explanation\": \"str\", \"similar\": [\"true\", \"false\"]},\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )        \n",
    "        logger.debug(f\"ScoreStage Results {results}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def preprocess(self, obj: PromptObject):\n",
    "        obj.prompt = make_llama_3_prompt(**self.make_prompt(obj))\n",
    "        logger.info(f\"Scoring Stage Prompt:\\n{obj.prompt}\")\n",
    "\n",
    "    def postprocess(self, obj: PromptObject):\n",
    "        logger.info(f\"Postprocess\")\n",
    "        obj.data['is_matching'] = self.is_matching(obj.data, obj.response)\n",
    "        obj.data['explanation'] = obj.response[\"explanation\"]\n",
    "        obj.data['similar'] = obj.response[\"similar\"] == \"true\"\n",
    "\n",
    "\n",
    "    def is_matching(self, data, response):\n",
    "        return (str(data.get('df',\"None\")).lower() == str(data['reference_df']).lower() \n",
    "                or response['similar'] == \"true\")\n",
    "\n",
    "    def make_prompt(self, obj: PromptObject):\n",
    "        # Your evaluation model compares SQL output from the generated and reference SQL queries, using another LLM in the pipeline\n",
    "        system_prompt = \"Compare the following two dataframes. They are similar if they are almost identical, or if they convey the same information about the nba_roster dataset\"\n",
    "        system_prompt += \"Respond with valid JSON {'explanation' : str, 'similar' : bool}\"\n",
    "        user_prompt = (\n",
    "            f\"========== Dataframe 1 =========\\n{str(obj.data.get('df','None')).lower()}\\n\\n\"\n",
    "        )\n",
    "        user_prompt += (\n",
    "            f\"========== Dataframe 2 =========\\n{str(obj.data['reference_df']).lower()}\\n\\n\"\n",
    "        )\n",
    "        user_prompt += f\"Can you tell me if these dataframes are similar?\"\n",
    "        return {\n",
    "            \"system\": system_prompt,\n",
    "            \"user\": user_prompt\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d1645d-d59e-4cc5-a13d-a33758745ac4",
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": [
    "class EvaluationPipeline(GenerationPipeline):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.query_stage = QueryStage(args.sql_model_name)\n",
    "        self.score_stage = ScoreStage()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.query_stage(x)\n",
    "        x = self.score_stage(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c3ae67-ce62-470f-bdf6-043c409dd555",
   "metadata": {
    "height": 251
   },
   "outputs": [],
   "source": [
    "async def run_eval(dataset, args):\n",
    "    results = await run_evaluation_pipeline(dataset, args)\n",
    "    print(\"Total results:\", len(results))\n",
    "    return results\n",
    "\n",
    "async def run_evaluation_pipeline(dataset, args):\n",
    "    results = EvaluationPipeline(args).call(dataset)\n",
    "    result_list = []\n",
    "\n",
    "    pbar = tqdm(desc=\"Saving results\", unit=\" results\")\n",
    "    async for result in results:\n",
    "        result_list.append(result)\n",
    "        pbar.update()\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa784583-b6e8-4145-870f-858b72f554c3",
   "metadata": {
    "height": 1271
   },
   "outputs": [],
   "source": [
    "def save_eval_results(results, args):\n",
    "    base_path = \"./data/results\"\n",
    "    now = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "    experiment_name = f\"nba_sql_pipeline_{now}\"\n",
    "    experiment_dir = os.path.join(base_path, experiment_name)\n",
    "    os.makedirs(os.path.join(base_path, experiment_name))\n",
    "\n",
    "    # Write args to file\n",
    "    args_file_name = f\"{experiment_dir}/args.txt\"\n",
    "    with open(args_file_name, \"w\") as writer:\n",
    "        pprint(args.__dict__, writer)\n",
    "\n",
    "\n",
    "    def is_correct(r):\n",
    "        if (\n",
    "            (r.data[\"query_succeeded\"] and r.data['is_matching']) or \n",
    "            r.data[\"generated_query\"] == r.data['sql']\n",
    "        ):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    # Write sql results and errors to file\n",
    "    results_file_name = f\"{experiment_dir}/sql_results.jsonl\"\n",
    "    with jsonlines.open(results_file_name, \"w\") as writer:\n",
    "        for result in results:\n",
    "            if not is_correct(result):\n",
    "                continue\n",
    "            writer.write(\n",
    "                {\n",
    "                    \"question\": result.data['question'],\n",
    "                    \"query\": result.data[\"generated_query\"],\n",
    "                    \"query_succeeded\": result.data[\"query_succeeded\"],\n",
    "                    \"reference_sql\": result.data['sql'],\n",
    "                    \"df\": str(result.data.get('df', 'None')),\n",
    "                    \"reference_df\": str(result.data['reference_df']),\n",
    "                    'is_matching': result.data['is_matching'],\n",
    "                    'similar': result.data['similar'],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    results_file_name = f\"{experiment_dir}/sql_errors.jsonl\"\n",
    "    with jsonlines.open(results_file_name, \"w\") as writer:\n",
    "        for result in results:\n",
    "            if is_correct(result):\n",
    "                continue\n",
    "            writer.write(\n",
    "                {\n",
    "                    \"question\": result.data['question'],\n",
    "                    \"query\": result.data[\"generated_query\"],\n",
    "                    \"query_succeeded\": result.data[\"query_succeeded\"],\n",
    "                    \"df\": str(result.data.get('df', 'None')),\n",
    "                    \"reference_df\": str(result.data['reference_df']),\n",
    "                    'is_matching': result.data['is_matching'],\n",
    "                    'similar': result.data['similar'],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Write statistics to file\n",
    "    average_sql_succeeded = sum(\n",
    "        [result.data[\"query_succeeded\"] for result in results]\n",
    "    ) / len(results)\n",
    "    average_correct = sum(\n",
    "        [result.data[\"query_succeeded\"] and result.data['is_matching'] for result in results]\n",
    "    ) / len(results)\n",
    "\n",
    "    file_name = f\"{experiment_dir}/summary.txt\"\n",
    "    with open(file_name, \"w\") as writer:\n",
    "        print(f\"Total size of eval dataset: {len(results)}\", file=writer)\n",
    "        print(f\"Total size of eval dataset: {len(results)}\")\n",
    "        print(f\"Percent Valid SQL Syntax: {average_sql_succeeded*100}\", file=writer)\n",
    "        print(f\"Percent Valid SQL Syntax: {average_sql_succeeded*100}\")\n",
    "        print(f\"Percent Correct SQL Query: {average_correct*100}\", file=writer)\n",
    "        print(f\"Percent Correct SQL Query: {average_correct*100}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb9227b-0d44-4dcf-8090-489821509ed2",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "args = Args()\n",
    "dataset = load_gold_dataset(args)\n",
    "results = await run_eval(dataset, args)\n",
    "save_eval_results(results, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446dc37c-a14b-4890-8bdd-2f69c389dda3",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
